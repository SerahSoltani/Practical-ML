{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Named_Entity_Recognition_Mandarin_Weibo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIsYTir1LdWd"
      },
      "source": [
        "# Named Entity Recognition in Mandarin on a Weibo Social Media Dataset\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "[Github](https://github.com/eugenesiow/practical-ml/blob/master/notebooks/Named_Entity_Recognition_Mandarin_Weibo.ipynb) | More Notebooks @ [eugenesiow/practical-ml](https://github.com/eugenesiow/practical-ml)\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wEHlNdHL8Nv"
      },
      "source": [
        "Notebook to train a [flair](https://github.com/flairNLP/flair) model in mandarin using stacked embeddings (with word and BERT embeddings) to perform named entity recognition (NER). \r\n",
        "\r\n",
        "The [dataset](https://github.com/hltcoe/golden-horse) used contains 1,890 Sina Weibo messages annotated with four entity types (person, organization, location and geo-political entity), including named and nominal mentions from the paper [Peng et al. (2015)](https://www.aclweb.org/anthology/D15-1064/) and with revised annotated data from [He et al. (2016)](https://arxiv.org/abs/1611.04234).\r\n",
        "\r\n",
        "The current state-of-the-art model on this dataset is from [Peng et al. (2016)](https://www.aclweb.org/anthology/P16-2025/) with an average F1-score of **47.0%** (Table 1) and from [Peng et al. (2015)](https://www.aclweb.org/anthology/D15-1064.pdf) with an F1-score of **44.1%** (Table 2). The authors say that the poor results on the test set show the \"difficulty of this task\" - which is true a sense because the dataset is really quite small for the NER task with 4 classes (x2 as they differentiate nominal and named entities) with a test set of only 270 sentences.\r\n",
        "\r\n",
        "Our flair model is able to improve the state-of-the-art with an F1-score of **67.5%**, which is a cool 20+ absolute percentage points better than the current state-of-the-art performance.\r\n",
        "\r\n",
        "The notebook is structured as follows:\r\n",
        "* Setting up the GPU Environment\r\n",
        "* Getting Data\r\n",
        "* Training and Testing the Model\r\n",
        "* Using the Model (Running Inference)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YbAAbVsOLyp"
      },
      "source": [
        "## Task Description\r\n",
        "\r\n",
        "> Named entity recognition (NER) is the task of tagging entities in text with their corresponding type. Approaches typically use BIO notation, which differentiates the beginning (B) and the inside (I) of entities. O is used for non-entity tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibsCfqKxPwZl"
      },
      "source": [
        "# Setting up the GPU Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EniXBtDPyEZ"
      },
      "source": [
        "#### Ensure we have a GPU runtime\r\n",
        "\r\n",
        "If you're running this notebook in Google Colab, select `Runtime` > `Change Runtime Type` from the menubar. Ensure that `GPU` is selected as the `Hardware accelerator`. This will allow us to use the GPU to train the model subsequently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYu2HDP_P0e5"
      },
      "source": [
        "#### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5pgQDNVB20K",
        "outputId": "71d186df-f687-46eb-a6e2-1a574e02bb96"
      },
      "source": [
        "pip install -q flair"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 450kB 22.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 798kB 59.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 52.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 55.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 19.7MB 1.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 983kB 52.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 39.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 52.4MB/s \n",
            "\u001b[?25h  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI558IY2P94A"
      },
      "source": [
        "# Getting Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOMnhOfYP_lw"
      },
      "source": [
        "The dataset, including the train, test and dev sets, has just been included in the `0.7 release` of flair, hence, we just use the `flair.datasets` loader to load the `WEIBO_NER` dataset into the flair `Corpus`. The [raw datasets](https://github.com/87302380/WEIBO_NER) are also available on Github."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRMA0FcZLuEF",
        "outputId": "1fee6bc5-b6ed-47b2-8dfc-942af5ad4e54"
      },
      "source": [
        "import flair.datasets\r\n",
        "from flair.data import Corpus\r\n",
        "corpus = flair.datasets.WEIBO_NER()\r\n",
        "print(corpus)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-23 06:35:01,004 Reading data from /root/.flair/datasets/weibo_ner\n",
            "2020-12-23 06:35:01,009 Train: /root/.flair/datasets/weibo_ner/weiboNER_2nd_conll_format.train\n",
            "2020-12-23 06:35:01,009 Dev: /root/.flair/datasets/weibo_ner/weiboNER_2nd_conll_format.dev\n",
            "2020-12-23 06:35:01,010 Test: /root/.flair/datasets/weibo_ner/weiboNER_2nd_conll_format.test\n",
            "Corpus: 1350 train + 270 dev + 270 test sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY12224EPHqu"
      },
      "source": [
        "We can see that the total 1,890 sentences have already been split into train (1,350), dev (270) and test (270) sets in a 5:1:1 ratio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ltHe5_vR1mw"
      },
      "source": [
        "# Training and Testing the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H4kP93rR8vA"
      },
      "source": [
        "#### Train the Model\r\n",
        "\r\n",
        "To train the flair `SequenceTagger`, we use the `ModelTrainer` object with the corpus and the tagger to be trained. We use flair's sensible default options in the `.train()` method, while specifying the output folder for the `SequenceTagger` model to be `/content/model/`. We also set the `embeddings_storage_mode` to be `gpu` to utilise the GPU to store the embeddings for more speed. Note that if you run this with a larger dataset you might run out of GPU memory, so be sure to set this option to `cpu` - it will still use the GPU to train but the embeddings will not be stored in the CPU and there will be a transfer to the GPU each epoch.\r\n",
        "\r\n",
        "Be prepared to allow the training to run for about 0.5 to 1 hour. We set the `max_epochs` to 50 so the the training will complete faster, for higher F1-score you can increase this number to 100 or 150."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-cFJ-7iR8mu",
        "outputId": "e03254d4-6d4d-460c-9f9c-57e0d2a6cd32"
      },
      "source": [
        "import flair\r\n",
        "from typing import List\r\n",
        "from flair.trainers import ModelTrainer\r\n",
        "from flair.models import SequenceTagger\r\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, BertEmbeddings, BytePairEmbeddings\r\n",
        "\r\n",
        "tag_type = 'ner'\r\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\r\n",
        "\r\n",
        "# For an even faster training time, you can comment out the BytePairEmbeddings\r\n",
        "# Note: there will be a small drop in performance if you do so.\r\n",
        "embedding_types: List[TokenEmbeddings] = [\r\n",
        "    WordEmbeddings('zh-crawl'),\r\n",
        "    BytePairEmbeddings('zh'),\r\n",
        "    BertEmbeddings('bert-base-chinese'),\r\n",
        "]\r\n",
        "\r\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\r\n",
        "\r\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\r\n",
        "                                        embeddings=embeddings,\r\n",
        "                                        tag_dictionary=tag_dictionary,\r\n",
        "                                        tag_type=tag_type,\r\n",
        "                                        use_crf=True)\r\n",
        "\r\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\r\n",
        "\r\n",
        "trainer.train('/content/model/',\r\n",
        "              learning_rate=0.1,\r\n",
        "              mini_batch_size=32,\r\n",
        "              max_epochs=50,\r\n",
        "              embeddings_storage_mode='gpu')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated method __init__. (Use 'TransformerWordEmbeddings' for all transformer-based word embeddings) -- Deprecated since version 0.4.5.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-23 06:35:34,075 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:35:34,078 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): WordEmbeddings('zh-crawl')\n",
            "    (list_embedding_1): BytePairEmbeddings(model=1-bpe-zh-100000-50)\n",
            "    (list_embedding_2): BertEmbeddings(\n",
            "      (model): BertModel(\n",
            "        (embeddings): BertEmbeddings(\n",
            "          (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder): BertEncoder(\n",
            "          (layer): ModuleList(\n",
            "            (0): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (1): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (2): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (3): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (4): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (5): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (6): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (7): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (8): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (9): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (10): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (11): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (pooler): BertPooler(\n",
            "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (activation): Tanh()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=3472, out_features=3472, bias=True)\n",
            "  (rnn): LSTM(3472, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=31, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2020-12-23 06:35:34,079 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:35:34,081 Corpus: \"Corpus: 1350 train + 270 dev + 270 test sentences\"\n",
            "2020-12-23 06:35:34,082 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:35:34,083 Parameters:\n",
            "2020-12-23 06:35:34,084  - learning_rate: \"0.1\"\n",
            "2020-12-23 06:35:34,086  - mini_batch_size: \"32\"\n",
            "2020-12-23 06:35:34,093  - patience: \"3\"\n",
            "2020-12-23 06:35:34,094  - anneal_factor: \"0.5\"\n",
            "2020-12-23 06:35:34,096  - max_epochs: \"50\"\n",
            "2020-12-23 06:35:34,098  - shuffle: \"True\"\n",
            "2020-12-23 06:35:34,099  - train_with_dev: \"False\"\n",
            "2020-12-23 06:35:34,101  - batch_growth_annealing: \"False\"\n",
            "2020-12-23 06:35:34,103 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:35:34,105 Model training base path: \"/content/model\"\n",
            "2020-12-23 06:35:34,106 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:35:34,108 Device: cuda:0\n",
            "2020-12-23 06:35:34,110 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:35:34,112 Embeddings storage mode: gpu\n",
            "2020-12-23 06:35:34,122 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:35:37,645 epoch 1 - iter 4/43 - loss 144.22977829 - samples/sec: 36.38 - lr: 0.100000\n",
            "2020-12-23 06:35:41,117 epoch 1 - iter 8/43 - loss 91.06989884 - samples/sec: 36.90 - lr: 0.100000\n",
            "2020-12-23 06:35:44,549 epoch 1 - iter 12/43 - loss 70.64791473 - samples/sec: 37.32 - lr: 0.100000\n",
            "2020-12-23 06:35:47,829 epoch 1 - iter 16/43 - loss 60.18261242 - samples/sec: 39.04 - lr: 0.100000\n",
            "2020-12-23 06:35:51,182 epoch 1 - iter 20/43 - loss 54.85139446 - samples/sec: 38.19 - lr: 0.100000\n",
            "2020-12-23 06:35:54,287 epoch 1 - iter 24/43 - loss 49.90954932 - samples/sec: 41.25 - lr: 0.100000\n",
            "2020-12-23 06:35:57,781 epoch 1 - iter 28/43 - loss 46.92953280 - samples/sec: 36.65 - lr: 0.100000\n",
            "2020-12-23 06:36:01,006 epoch 1 - iter 32/43 - loss 43.48488683 - samples/sec: 39.70 - lr: 0.100000\n",
            "2020-12-23 06:36:04,632 epoch 1 - iter 36/43 - loss 41.90378565 - samples/sec: 35.31 - lr: 0.100000\n",
            "2020-12-23 06:36:08,217 epoch 1 - iter 40/43 - loss 40.01684928 - samples/sec: 35.72 - lr: 0.100000\n",
            "2020-12-23 06:36:10,127 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:36:10,128 EPOCH 1 done: loss 39.3918 - lr 0.1000000\n",
            "2020-12-23 06:36:14,846 DEV : loss 15.872177124023438 - score 0.0049\n",
            "2020-12-23 06:36:14,854 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:36:39,818 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:36:42,207 epoch 2 - iter 4/43 - loss 20.51611304 - samples/sec: 69.91 - lr: 0.100000\n",
            "2020-12-23 06:36:43,597 epoch 2 - iter 8/43 - loss 20.76537073 - samples/sec: 92.20 - lr: 0.100000\n",
            "2020-12-23 06:36:45,212 epoch 2 - iter 12/43 - loss 21.62938015 - samples/sec: 79.43 - lr: 0.100000\n",
            "2020-12-23 06:36:46,844 epoch 2 - iter 16/43 - loss 20.90195841 - samples/sec: 78.46 - lr: 0.100000\n",
            "2020-12-23 06:36:48,533 epoch 2 - iter 20/43 - loss 20.95215564 - samples/sec: 75.90 - lr: 0.100000\n",
            "2020-12-23 06:36:50,071 epoch 2 - iter 24/43 - loss 20.43731864 - samples/sec: 83.28 - lr: 0.100000\n",
            "2020-12-23 06:36:51,634 epoch 2 - iter 28/43 - loss 20.59251806 - samples/sec: 82.03 - lr: 0.100000\n",
            "2020-12-23 06:36:53,280 epoch 2 - iter 32/43 - loss 20.47852021 - samples/sec: 77.88 - lr: 0.100000\n",
            "2020-12-23 06:36:54,934 epoch 2 - iter 36/43 - loss 20.40846533 - samples/sec: 77.46 - lr: 0.100000\n",
            "2020-12-23 06:36:56,602 epoch 2 - iter 40/43 - loss 20.62114553 - samples/sec: 76.81 - lr: 0.100000\n",
            "2020-12-23 06:36:57,735 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:36:57,736 EPOCH 2 done: loss 20.2394 - lr 0.1000000\n",
            "2020-12-23 06:36:58,793 DEV : loss 17.027008056640625 - score 0.005\n",
            "2020-12-23 06:36:58,800 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:37:24,012 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:37:25,769 epoch 3 - iter 4/43 - loss 21.36505961 - samples/sec: 74.16 - lr: 0.100000\n",
            "2020-12-23 06:37:27,266 epoch 3 - iter 8/43 - loss 20.34303200 - samples/sec: 85.56 - lr: 0.100000\n",
            "2020-12-23 06:37:28,965 epoch 3 - iter 12/43 - loss 20.32099557 - samples/sec: 75.47 - lr: 0.100000\n",
            "2020-12-23 06:37:30,443 epoch 3 - iter 16/43 - loss 18.86545974 - samples/sec: 86.73 - lr: 0.100000\n",
            "2020-12-23 06:37:31,996 epoch 3 - iter 20/43 - loss 19.04173350 - samples/sec: 82.46 - lr: 0.100000\n",
            "2020-12-23 06:37:33,551 epoch 3 - iter 24/43 - loss 18.13513851 - samples/sec: 82.38 - lr: 0.100000\n",
            "2020-12-23 06:37:35,119 epoch 3 - iter 28/43 - loss 17.35183849 - samples/sec: 81.71 - lr: 0.100000\n",
            "2020-12-23 06:37:36,759 epoch 3 - iter 32/43 - loss 17.30418277 - samples/sec: 78.13 - lr: 0.100000\n",
            "2020-12-23 06:37:38,273 epoch 3 - iter 36/43 - loss 16.67987733 - samples/sec: 84.57 - lr: 0.100000\n",
            "2020-12-23 06:37:39,876 epoch 3 - iter 40/43 - loss 16.44120653 - samples/sec: 79.92 - lr: 0.100000\n",
            "2020-12-23 06:37:40,920 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:37:40,921 EPOCH 3 done: loss 16.0304 - lr 0.1000000\n",
            "2020-12-23 06:37:41,958 DEV : loss 9.663846015930176 - score 0.225\n",
            "2020-12-23 06:37:41,964 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:38:07,804 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:38:09,448 epoch 4 - iter 4/43 - loss 15.11681700 - samples/sec: 78.02 - lr: 0.100000\n",
            "2020-12-23 06:38:11,035 epoch 4 - iter 8/43 - loss 14.37938356 - samples/sec: 80.85 - lr: 0.100000\n",
            "2020-12-23 06:38:12,602 epoch 4 - iter 12/43 - loss 14.22715163 - samples/sec: 81.73 - lr: 0.100000\n",
            "2020-12-23 06:38:14,129 epoch 4 - iter 16/43 - loss 13.43213195 - samples/sec: 83.92 - lr: 0.100000\n",
            "2020-12-23 06:38:15,741 epoch 4 - iter 20/43 - loss 13.91348615 - samples/sec: 79.43 - lr: 0.100000\n",
            "2020-12-23 06:38:17,294 epoch 4 - iter 24/43 - loss 13.50380921 - samples/sec: 82.51 - lr: 0.100000\n",
            "2020-12-23 06:38:18,835 epoch 4 - iter 28/43 - loss 13.31979370 - samples/sec: 83.14 - lr: 0.100000\n",
            "2020-12-23 06:38:20,416 epoch 4 - iter 32/43 - loss 12.94199556 - samples/sec: 81.08 - lr: 0.100000\n",
            "2020-12-23 06:38:22,026 epoch 4 - iter 36/43 - loss 12.88623375 - samples/sec: 79.58 - lr: 0.100000\n",
            "2020-12-23 06:38:23,702 epoch 4 - iter 40/43 - loss 12.90845039 - samples/sec: 76.45 - lr: 0.100000\n",
            "2020-12-23 06:38:24,775 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:38:24,777 EPOCH 4 done: loss 12.8174 - lr 0.1000000\n",
            "2020-12-23 06:38:25,792 DEV : loss 6.951746463775635 - score 0.5\n",
            "2020-12-23 06:38:25,800 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:38:51,240 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:38:55,317 epoch 5 - iter 4/43 - loss 10.39647079 - samples/sec: 80.91 - lr: 0.100000\n",
            "2020-12-23 06:38:56,963 epoch 5 - iter 8/43 - loss 10.71643221 - samples/sec: 77.86 - lr: 0.100000\n",
            "2020-12-23 06:38:58,503 epoch 5 - iter 12/43 - loss 11.06355619 - samples/sec: 83.31 - lr: 0.100000\n",
            "2020-12-23 06:38:59,937 epoch 5 - iter 16/43 - loss 10.26898101 - samples/sec: 89.53 - lr: 0.100000\n",
            "2020-12-23 06:39:01,520 epoch 5 - iter 20/43 - loss 9.94426811 - samples/sec: 81.10 - lr: 0.100000\n",
            "2020-12-23 06:39:03,065 epoch 5 - iter 24/43 - loss 9.68968147 - samples/sec: 82.89 - lr: 0.100000\n",
            "2020-12-23 06:39:04,736 epoch 5 - iter 28/43 - loss 9.85106204 - samples/sec: 76.78 - lr: 0.100000\n",
            "2020-12-23 06:39:06,307 epoch 5 - iter 32/43 - loss 9.73777924 - samples/sec: 81.55 - lr: 0.100000\n",
            "2020-12-23 06:39:08,009 epoch 5 - iter 36/43 - loss 10.04469928 - samples/sec: 75.26 - lr: 0.100000\n",
            "2020-12-23 06:39:09,586 epoch 5 - iter 40/43 - loss 10.05620779 - samples/sec: 81.19 - lr: 0.100000\n",
            "2020-12-23 06:39:10,670 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:39:10,673 EPOCH 5 done: loss 10.0896 - lr 0.1000000\n",
            "2020-12-23 06:39:12,595 DEV : loss 6.2898736000061035 - score 0.5691\n",
            "2020-12-23 06:39:12,601 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:39:35,021 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:39:36,665 epoch 6 - iter 4/43 - loss 8.35844135 - samples/sec: 78.91 - lr: 0.100000\n",
            "2020-12-23 06:39:38,299 epoch 6 - iter 8/43 - loss 8.52188766 - samples/sec: 78.44 - lr: 0.100000\n",
            "2020-12-23 06:39:39,991 epoch 6 - iter 12/43 - loss 8.69721214 - samples/sec: 75.72 - lr: 0.100000\n",
            "2020-12-23 06:39:41,578 epoch 6 - iter 16/43 - loss 8.58831370 - samples/sec: 80.73 - lr: 0.100000\n",
            "2020-12-23 06:39:43,051 epoch 6 - iter 20/43 - loss 8.57167432 - samples/sec: 87.09 - lr: 0.100000\n",
            "2020-12-23 06:39:44,751 epoch 6 - iter 24/43 - loss 8.67784582 - samples/sec: 75.35 - lr: 0.100000\n",
            "2020-12-23 06:39:46,317 epoch 6 - iter 28/43 - loss 8.58713853 - samples/sec: 81.81 - lr: 0.100000\n",
            "2020-12-23 06:39:47,946 epoch 6 - iter 32/43 - loss 8.65947048 - samples/sec: 78.63 - lr: 0.100000\n",
            "2020-12-23 06:39:49,472 epoch 6 - iter 36/43 - loss 8.69388966 - samples/sec: 84.04 - lr: 0.100000\n",
            "2020-12-23 06:39:50,994 epoch 6 - iter 40/43 - loss 8.62564567 - samples/sec: 84.29 - lr: 0.100000\n",
            "2020-12-23 06:39:51,912 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:39:51,913 EPOCH 6 done: loss 8.6163 - lr 0.1000000\n",
            "2020-12-23 06:39:52,897 DEV : loss 5.883848667144775 - score 0.5458\n",
            "2020-12-23 06:39:52,905 BAD EPOCHS (no improvement): 1\n",
            "2020-12-23 06:39:52,909 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:39:54,453 epoch 7 - iter 4/43 - loss 8.52589977 - samples/sec: 82.98 - lr: 0.100000\n",
            "2020-12-23 06:39:56,024 epoch 7 - iter 8/43 - loss 8.95510483 - samples/sec: 81.56 - lr: 0.100000\n",
            "2020-12-23 06:39:57,722 epoch 7 - iter 12/43 - loss 8.96793040 - samples/sec: 75.57 - lr: 0.100000\n",
            "2020-12-23 06:39:59,244 epoch 7 - iter 16/43 - loss 8.84204632 - samples/sec: 84.15 - lr: 0.100000\n",
            "2020-12-23 06:40:00,931 epoch 7 - iter 20/43 - loss 8.83163519 - samples/sec: 75.93 - lr: 0.100000\n",
            "2020-12-23 06:40:02,474 epoch 7 - iter 24/43 - loss 8.46217936 - samples/sec: 83.10 - lr: 0.100000\n",
            "2020-12-23 06:40:04,007 epoch 7 - iter 28/43 - loss 8.29485367 - samples/sec: 83.67 - lr: 0.100000\n",
            "2020-12-23 06:40:05,587 epoch 7 - iter 32/43 - loss 8.12294389 - samples/sec: 81.10 - lr: 0.100000\n",
            "2020-12-23 06:40:07,237 epoch 7 - iter 36/43 - loss 8.24393545 - samples/sec: 77.64 - lr: 0.100000\n",
            "2020-12-23 06:40:08,965 epoch 7 - iter 40/43 - loss 8.01004871 - samples/sec: 74.16 - lr: 0.100000\n",
            "2020-12-23 06:40:09,905 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:40:09,906 EPOCH 7 done: loss 8.0011 - lr 0.1000000\n",
            "2020-12-23 06:40:10,953 DEV : loss 4.86557149887085 - score 0.6014\n",
            "2020-12-23 06:40:10,959 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:40:33,189 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:40:34,822 epoch 8 - iter 4/43 - loss 7.07219851 - samples/sec: 78.72 - lr: 0.100000\n",
            "2020-12-23 06:40:36,446 epoch 8 - iter 8/43 - loss 6.86064345 - samples/sec: 79.22 - lr: 0.100000\n",
            "2020-12-23 06:40:37,961 epoch 8 - iter 12/43 - loss 6.78169676 - samples/sec: 84.55 - lr: 0.100000\n",
            "2020-12-23 06:40:39,635 epoch 8 - iter 16/43 - loss 6.54739967 - samples/sec: 76.52 - lr: 0.100000\n",
            "2020-12-23 06:40:41,175 epoch 8 - iter 20/43 - loss 6.20493726 - samples/sec: 83.17 - lr: 0.100000\n",
            "2020-12-23 06:40:42,738 epoch 8 - iter 24/43 - loss 6.62896710 - samples/sec: 81.96 - lr: 0.100000\n",
            "2020-12-23 06:40:44,435 epoch 8 - iter 28/43 - loss 6.99197891 - samples/sec: 75.49 - lr: 0.100000\n",
            "2020-12-23 06:40:46,194 epoch 8 - iter 32/43 - loss 7.16324823 - samples/sec: 72.87 - lr: 0.100000\n",
            "2020-12-23 06:40:47,783 epoch 8 - iter 36/43 - loss 7.28070097 - samples/sec: 80.61 - lr: 0.100000\n",
            "2020-12-23 06:40:49,361 epoch 8 - iter 40/43 - loss 7.21179810 - samples/sec: 81.26 - lr: 0.100000\n",
            "2020-12-23 06:40:50,291 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:40:50,291 EPOCH 8 done: loss 7.2035 - lr 0.1000000\n",
            "2020-12-23 06:40:51,321 DEV : loss 4.57301664352417 - score 0.6495\n",
            "2020-12-23 06:40:51,328 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:41:17,144 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:41:20,106 epoch 9 - iter 4/43 - loss 9.24102926 - samples/sec: 78.26 - lr: 0.100000\n",
            "2020-12-23 06:41:21,635 epoch 9 - iter 8/43 - loss 7.15426007 - samples/sec: 83.92 - lr: 0.100000\n",
            "2020-12-23 06:41:23,142 epoch 9 - iter 12/43 - loss 7.55110500 - samples/sec: 85.01 - lr: 0.100000\n",
            "2020-12-23 06:41:24,833 epoch 9 - iter 16/43 - loss 7.47627021 - samples/sec: 75.78 - lr: 0.100000\n",
            "2020-12-23 06:41:26,424 epoch 9 - iter 20/43 - loss 7.25390033 - samples/sec: 80.56 - lr: 0.100000\n",
            "2020-12-23 06:41:28,001 epoch 9 - iter 24/43 - loss 7.21520389 - samples/sec: 81.19 - lr: 0.100000\n",
            "2020-12-23 06:41:29,605 epoch 9 - iter 28/43 - loss 7.24934085 - samples/sec: 79.89 - lr: 0.100000\n",
            "2020-12-23 06:41:31,179 epoch 9 - iter 32/43 - loss 7.14800128 - samples/sec: 81.41 - lr: 0.100000\n",
            "2020-12-23 06:41:32,853 epoch 9 - iter 36/43 - loss 7.05114859 - samples/sec: 76.54 - lr: 0.100000\n",
            "2020-12-23 06:41:34,463 epoch 9 - iter 40/43 - loss 6.95610955 - samples/sec: 79.54 - lr: 0.100000\n",
            "2020-12-23 06:41:35,383 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:41:35,385 EPOCH 9 done: loss 6.8311 - lr 0.1000000\n",
            "2020-12-23 06:41:36,400 DEV : loss 4.379792213439941 - score 0.6607\n",
            "2020-12-23 06:41:36,407 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:42:02,384 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:42:04,671 epoch 10 - iter 4/43 - loss 6.06741774 - samples/sec: 79.02 - lr: 0.100000\n",
            "2020-12-23 06:42:06,139 epoch 10 - iter 8/43 - loss 5.96938884 - samples/sec: 87.64 - lr: 0.100000\n",
            "2020-12-23 06:42:07,797 epoch 10 - iter 12/43 - loss 6.07690972 - samples/sec: 77.27 - lr: 0.100000\n",
            "2020-12-23 06:42:09,306 epoch 10 - iter 16/43 - loss 6.27010496 - samples/sec: 84.94 - lr: 0.100000\n",
            "2020-12-23 06:42:10,859 epoch 10 - iter 20/43 - loss 6.26011792 - samples/sec: 82.50 - lr: 0.100000\n",
            "2020-12-23 06:42:12,340 epoch 10 - iter 24/43 - loss 6.30750011 - samples/sec: 86.49 - lr: 0.100000\n",
            "2020-12-23 06:42:14,101 epoch 10 - iter 28/43 - loss 6.89783651 - samples/sec: 72.73 - lr: 0.100000\n",
            "2020-12-23 06:42:15,748 epoch 10 - iter 32/43 - loss 6.87991809 - samples/sec: 77.78 - lr: 0.100000\n",
            "2020-12-23 06:42:17,408 epoch 10 - iter 36/43 - loss 6.76595405 - samples/sec: 77.19 - lr: 0.100000\n",
            "2020-12-23 06:42:18,859 epoch 10 - iter 40/43 - loss 6.68850651 - samples/sec: 88.24 - lr: 0.100000\n",
            "2020-12-23 06:42:19,866 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:42:19,869 EPOCH 10 done: loss 6.4763 - lr 0.1000000\n",
            "2020-12-23 06:42:20,882 DEV : loss 4.182097911834717 - score 0.6639\n",
            "2020-12-23 06:42:20,888 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:42:46,031 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:42:47,674 epoch 11 - iter 4/43 - loss 6.69029647 - samples/sec: 78.22 - lr: 0.100000\n",
            "2020-12-23 06:42:49,288 epoch 11 - iter 8/43 - loss 5.88464531 - samples/sec: 79.46 - lr: 0.100000\n",
            "2020-12-23 06:42:50,826 epoch 11 - iter 12/43 - loss 5.59960105 - samples/sec: 83.49 - lr: 0.100000\n",
            "2020-12-23 06:42:52,285 epoch 11 - iter 16/43 - loss 5.38141744 - samples/sec: 87.80 - lr: 0.100000\n",
            "2020-12-23 06:42:53,909 epoch 11 - iter 20/43 - loss 5.82888054 - samples/sec: 78.90 - lr: 0.100000\n",
            "2020-12-23 06:42:55,561 epoch 11 - iter 24/43 - loss 6.08405953 - samples/sec: 77.53 - lr: 0.100000\n",
            "2020-12-23 06:42:57,058 epoch 11 - iter 28/43 - loss 6.02853646 - samples/sec: 85.61 - lr: 0.100000\n",
            "2020-12-23 06:42:58,632 epoch 11 - iter 32/43 - loss 6.00464569 - samples/sec: 81.50 - lr: 0.100000\n",
            "2020-12-23 06:43:00,287 epoch 11 - iter 36/43 - loss 5.92787075 - samples/sec: 77.37 - lr: 0.100000\n",
            "2020-12-23 06:43:01,868 epoch 11 - iter 40/43 - loss 5.88228595 - samples/sec: 81.04 - lr: 0.100000\n",
            "2020-12-23 06:43:02,928 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:43:02,931 EPOCH 11 done: loss 5.9529 - lr 0.1000000\n",
            "2020-12-23 06:43:04,002 DEV : loss 3.9913041591644287 - score 0.6905\n",
            "2020-12-23 06:43:04,008 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:43:30,719 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:43:32,372 epoch 12 - iter 4/43 - loss 5.48121107 - samples/sec: 77.65 - lr: 0.100000\n",
            "2020-12-23 06:43:34,031 epoch 12 - iter 8/43 - loss 5.80044210 - samples/sec: 77.43 - lr: 0.100000\n",
            "2020-12-23 06:43:35,652 epoch 12 - iter 12/43 - loss 5.68327848 - samples/sec: 79.09 - lr: 0.100000\n",
            "2020-12-23 06:43:37,170 epoch 12 - iter 16/43 - loss 5.36818334 - samples/sec: 84.42 - lr: 0.100000\n",
            "2020-12-23 06:43:38,819 epoch 12 - iter 20/43 - loss 5.14263935 - samples/sec: 77.68 - lr: 0.100000\n",
            "2020-12-23 06:43:40,329 epoch 12 - iter 24/43 - loss 5.35406416 - samples/sec: 84.86 - lr: 0.100000\n",
            "2020-12-23 06:43:41,898 epoch 12 - iter 28/43 - loss 5.61690720 - samples/sec: 81.66 - lr: 0.100000\n",
            "2020-12-23 06:43:43,457 epoch 12 - iter 32/43 - loss 5.78185798 - samples/sec: 82.19 - lr: 0.100000\n",
            "2020-12-23 06:43:45,058 epoch 12 - iter 36/43 - loss 5.78682591 - samples/sec: 80.02 - lr: 0.100000\n",
            "2020-12-23 06:43:46,699 epoch 12 - iter 40/43 - loss 5.69560760 - samples/sec: 78.06 - lr: 0.100000\n",
            "2020-12-23 06:43:47,767 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:43:47,768 EPOCH 12 done: loss 5.5963 - lr 0.1000000\n",
            "2020-12-23 06:43:48,794 DEV : loss 3.9053568840026855 - score 0.7109\n",
            "2020-12-23 06:43:48,800 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:44:14,729 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:44:16,460 epoch 13 - iter 4/43 - loss 7.13879812 - samples/sec: 78.39 - lr: 0.100000\n",
            "2020-12-23 06:44:18,034 epoch 13 - iter 8/43 - loss 7.09174424 - samples/sec: 81.47 - lr: 0.100000\n",
            "2020-12-23 06:44:19,536 epoch 13 - iter 12/43 - loss 7.00968277 - samples/sec: 85.30 - lr: 0.100000\n",
            "2020-12-23 06:44:21,144 epoch 13 - iter 16/43 - loss 6.44084573 - samples/sec: 79.68 - lr: 0.100000\n",
            "2020-12-23 06:44:22,774 epoch 13 - iter 20/43 - loss 6.17349945 - samples/sec: 78.57 - lr: 0.100000\n",
            "2020-12-23 06:44:24,228 epoch 13 - iter 24/43 - loss 5.96514526 - samples/sec: 88.14 - lr: 0.100000\n",
            "2020-12-23 06:44:25,848 epoch 13 - iter 28/43 - loss 5.77588928 - samples/sec: 79.07 - lr: 0.100000\n",
            "2020-12-23 06:44:27,475 epoch 13 - iter 32/43 - loss 5.71054130 - samples/sec: 78.75 - lr: 0.100000\n",
            "2020-12-23 06:44:28,970 epoch 13 - iter 36/43 - loss 5.72442442 - samples/sec: 85.72 - lr: 0.100000\n",
            "2020-12-23 06:44:30,468 epoch 13 - iter 40/43 - loss 5.55599745 - samples/sec: 85.77 - lr: 0.100000\n",
            "2020-12-23 06:44:31,693 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:44:31,694 EPOCH 13 done: loss 5.5061 - lr 0.1000000\n",
            "2020-12-23 06:44:32,733 DEV : loss 3.975558042526245 - score 0.7089\n",
            "2020-12-23 06:44:32,740 BAD EPOCHS (no improvement): 1\n",
            "2020-12-23 06:44:32,741 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:44:34,345 epoch 14 - iter 4/43 - loss 4.67900056 - samples/sec: 79.95 - lr: 0.100000\n",
            "2020-12-23 06:44:35,881 epoch 14 - iter 8/43 - loss 4.66574636 - samples/sec: 83.40 - lr: 0.100000\n",
            "2020-12-23 06:44:37,554 epoch 14 - iter 12/43 - loss 4.97261538 - samples/sec: 76.62 - lr: 0.100000\n",
            "2020-12-23 06:44:39,024 epoch 14 - iter 16/43 - loss 5.05696820 - samples/sec: 87.17 - lr: 0.100000\n",
            "2020-12-23 06:44:40,669 epoch 14 - iter 20/43 - loss 5.13157855 - samples/sec: 77.88 - lr: 0.100000\n",
            "2020-12-23 06:44:42,248 epoch 14 - iter 24/43 - loss 5.58813623 - samples/sec: 81.11 - lr: 0.100000\n",
            "2020-12-23 06:44:43,763 epoch 14 - iter 28/43 - loss 5.62540823 - samples/sec: 84.61 - lr: 0.100000\n",
            "2020-12-23 06:44:45,327 epoch 14 - iter 32/43 - loss 5.58264082 - samples/sec: 81.94 - lr: 0.100000\n",
            "2020-12-23 06:44:47,071 epoch 14 - iter 36/43 - loss 5.38646842 - samples/sec: 73.44 - lr: 0.100000\n",
            "2020-12-23 06:44:48,580 epoch 14 - iter 40/43 - loss 5.39755868 - samples/sec: 84.88 - lr: 0.100000\n",
            "2020-12-23 06:44:49,514 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:44:49,519 EPOCH 14 done: loss 5.4233 - lr 0.1000000\n",
            "2020-12-23 06:44:50,579 DEV : loss 3.875393867492676 - score 0.699\n",
            "2020-12-23 06:44:50,586 BAD EPOCHS (no improvement): 2\n",
            "2020-12-23 06:44:50,587 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:44:52,172 epoch 15 - iter 4/43 - loss 4.80783629 - samples/sec: 80.97 - lr: 0.100000\n",
            "2020-12-23 06:44:53,756 epoch 15 - iter 8/43 - loss 5.53210157 - samples/sec: 80.86 - lr: 0.100000\n",
            "2020-12-23 06:44:55,327 epoch 15 - iter 12/43 - loss 5.44962037 - samples/sec: 81.66 - lr: 0.100000\n",
            "2020-12-23 06:44:56,899 epoch 15 - iter 16/43 - loss 5.24914461 - samples/sec: 81.50 - lr: 0.100000\n",
            "2020-12-23 06:44:58,416 epoch 15 - iter 20/43 - loss 5.27076018 - samples/sec: 84.57 - lr: 0.100000\n",
            "2020-12-23 06:45:00,003 epoch 15 - iter 24/43 - loss 5.18133198 - samples/sec: 80.77 - lr: 0.100000\n",
            "2020-12-23 06:45:01,635 epoch 15 - iter 28/43 - loss 5.34562707 - samples/sec: 78.47 - lr: 0.100000\n",
            "2020-12-23 06:45:03,263 epoch 15 - iter 32/43 - loss 5.18073942 - samples/sec: 78.70 - lr: 0.100000\n",
            "2020-12-23 06:45:04,776 epoch 15 - iter 36/43 - loss 5.15423430 - samples/sec: 84.67 - lr: 0.100000\n",
            "2020-12-23 06:45:06,419 epoch 15 - iter 40/43 - loss 4.94674361 - samples/sec: 77.94 - lr: 0.100000\n",
            "2020-12-23 06:45:07,458 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:45:07,459 EPOCH 15 done: loss 5.0568 - lr 0.1000000\n",
            "2020-12-23 06:45:08,487 DEV : loss 4.1406426429748535 - score 0.6979\n",
            "2020-12-23 06:45:08,494 BAD EPOCHS (no improvement): 3\n",
            "2020-12-23 06:45:08,495 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:45:10,039 epoch 16 - iter 4/43 - loss 4.71282810 - samples/sec: 83.08 - lr: 0.100000\n",
            "2020-12-23 06:45:11,610 epoch 16 - iter 8/43 - loss 4.53154197 - samples/sec: 81.60 - lr: 0.100000\n",
            "2020-12-23 06:45:13,285 epoch 16 - iter 12/43 - loss 4.43575408 - samples/sec: 76.50 - lr: 0.100000\n",
            "2020-12-23 06:45:14,919 epoch 16 - iter 16/43 - loss 4.85319085 - samples/sec: 78.39 - lr: 0.100000\n",
            "2020-12-23 06:45:16,408 epoch 16 - iter 20/43 - loss 4.76783583 - samples/sec: 86.04 - lr: 0.100000\n",
            "2020-12-23 06:45:18,077 epoch 16 - iter 24/43 - loss 4.64115416 - samples/sec: 76.96 - lr: 0.100000\n",
            "2020-12-23 06:45:19,635 epoch 16 - iter 28/43 - loss 4.64345848 - samples/sec: 82.27 - lr: 0.100000\n",
            "2020-12-23 06:45:21,254 epoch 16 - iter 32/43 - loss 4.69676013 - samples/sec: 79.10 - lr: 0.100000\n",
            "2020-12-23 06:45:22,807 epoch 16 - iter 36/43 - loss 4.60265883 - samples/sec: 82.49 - lr: 0.100000\n",
            "2020-12-23 06:45:24,387 epoch 16 - iter 40/43 - loss 4.65867288 - samples/sec: 81.08 - lr: 0.100000\n",
            "2020-12-23 06:45:25,309 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:45:25,311 EPOCH 16 done: loss 4.6944 - lr 0.1000000\n",
            "2020-12-23 06:45:26,332 DEV : loss 4.614320755004883 - score 0.6698\n",
            "Epoch    16: reducing learning rate of group 0 to 5.0000e-02.\n",
            "2020-12-23 06:45:26,339 BAD EPOCHS (no improvement): 4\n",
            "2020-12-23 06:45:26,340 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:45:27,986 epoch 17 - iter 4/43 - loss 5.27345204 - samples/sec: 77.97 - lr: 0.050000\n",
            "2020-12-23 06:45:29,509 epoch 17 - iter 8/43 - loss 3.90919366 - samples/sec: 84.12 - lr: 0.050000\n",
            "2020-12-23 06:45:31,197 epoch 17 - iter 12/43 - loss 4.33011329 - samples/sec: 75.90 - lr: 0.050000\n",
            "2020-12-23 06:45:32,803 epoch 17 - iter 16/43 - loss 4.23585655 - samples/sec: 79.77 - lr: 0.050000\n",
            "2020-12-23 06:45:34,328 epoch 17 - iter 20/43 - loss 4.09117967 - samples/sec: 83.98 - lr: 0.050000\n",
            "2020-12-23 06:45:35,988 epoch 17 - iter 24/43 - loss 3.98791450 - samples/sec: 77.17 - lr: 0.050000\n",
            "2020-12-23 06:45:37,529 epoch 17 - iter 28/43 - loss 3.87199145 - samples/sec: 83.26 - lr: 0.050000\n",
            "2020-12-23 06:45:39,018 epoch 17 - iter 32/43 - loss 3.92814337 - samples/sec: 86.13 - lr: 0.050000\n",
            "2020-12-23 06:45:40,466 epoch 17 - iter 36/43 - loss 3.98950063 - samples/sec: 88.50 - lr: 0.050000\n",
            "2020-12-23 06:45:42,008 epoch 17 - iter 40/43 - loss 4.01011748 - samples/sec: 83.09 - lr: 0.050000\n",
            "2020-12-23 06:45:43,184 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:45:43,186 EPOCH 17 done: loss 4.1333 - lr 0.0500000\n",
            "2020-12-23 06:45:44,201 DEV : loss 3.71307373046875 - score 0.7159\n",
            "2020-12-23 06:45:44,207 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:46:06,197 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:46:07,920 epoch 18 - iter 4/43 - loss 3.15091878 - samples/sec: 74.45 - lr: 0.050000\n",
            "2020-12-23 06:46:09,480 epoch 18 - iter 8/43 - loss 4.16329911 - samples/sec: 82.34 - lr: 0.050000\n",
            "2020-12-23 06:46:11,105 epoch 18 - iter 12/43 - loss 4.32774697 - samples/sec: 78.85 - lr: 0.050000\n",
            "2020-12-23 06:46:12,637 epoch 18 - iter 16/43 - loss 4.14160289 - samples/sec: 83.69 - lr: 0.050000\n",
            "2020-12-23 06:46:14,272 epoch 18 - iter 20/43 - loss 3.93657418 - samples/sec: 78.35 - lr: 0.050000\n",
            "2020-12-23 06:46:15,897 epoch 18 - iter 24/43 - loss 4.00837941 - samples/sec: 78.85 - lr: 0.050000\n",
            "2020-12-23 06:46:17,556 epoch 18 - iter 28/43 - loss 4.06547577 - samples/sec: 77.23 - lr: 0.050000\n",
            "2020-12-23 06:46:19,129 epoch 18 - iter 32/43 - loss 4.13808609 - samples/sec: 81.45 - lr: 0.050000\n",
            "2020-12-23 06:46:20,789 epoch 18 - iter 36/43 - loss 4.08038620 - samples/sec: 77.16 - lr: 0.050000\n",
            "2020-12-23 06:46:22,302 epoch 18 - iter 40/43 - loss 3.97571707 - samples/sec: 84.71 - lr: 0.050000\n",
            "2020-12-23 06:46:23,309 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:46:23,312 EPOCH 18 done: loss 3.9280 - lr 0.0500000\n",
            "2020-12-23 06:46:24,336 DEV : loss 3.8614141941070557 - score 0.7099\n",
            "2020-12-23 06:46:24,343 BAD EPOCHS (no improvement): 1\n",
            "2020-12-23 06:46:24,345 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:46:25,850 epoch 19 - iter 4/43 - loss 3.59112316 - samples/sec: 85.23 - lr: 0.050000\n",
            "2020-12-23 06:46:27,502 epoch 19 - iter 8/43 - loss 4.19730055 - samples/sec: 77.68 - lr: 0.050000\n",
            "2020-12-23 06:46:29,091 epoch 19 - iter 12/43 - loss 3.97228094 - samples/sec: 80.62 - lr: 0.050000\n",
            "2020-12-23 06:46:30,713 epoch 19 - iter 16/43 - loss 3.93058768 - samples/sec: 79.02 - lr: 0.050000\n",
            "2020-12-23 06:46:32,328 epoch 19 - iter 20/43 - loss 3.79837086 - samples/sec: 79.40 - lr: 0.050000\n",
            "2020-12-23 06:46:33,902 epoch 19 - iter 24/43 - loss 3.71810337 - samples/sec: 81.41 - lr: 0.050000\n",
            "2020-12-23 06:46:35,488 epoch 19 - iter 28/43 - loss 3.79663073 - samples/sec: 80.78 - lr: 0.050000\n",
            "2020-12-23 06:46:37,097 epoch 19 - iter 32/43 - loss 3.80306795 - samples/sec: 79.60 - lr: 0.050000\n",
            "2020-12-23 06:46:38,641 epoch 19 - iter 36/43 - loss 3.73124137 - samples/sec: 82.97 - lr: 0.050000\n",
            "2020-12-23 06:46:40,199 epoch 19 - iter 40/43 - loss 3.66778712 - samples/sec: 82.23 - lr: 0.050000\n",
            "2020-12-23 06:46:41,307 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:46:41,308 EPOCH 19 done: loss 3.6806 - lr 0.0500000\n",
            "2020-12-23 06:46:42,332 DEV : loss 3.892859697341919 - score 0.7009\n",
            "2020-12-23 06:46:42,339 BAD EPOCHS (no improvement): 2\n",
            "2020-12-23 06:46:42,340 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:46:43,901 epoch 20 - iter 4/43 - loss 3.80822778 - samples/sec: 82.21 - lr: 0.050000\n",
            "2020-12-23 06:46:45,401 epoch 20 - iter 8/43 - loss 4.28224522 - samples/sec: 85.42 - lr: 0.050000\n",
            "2020-12-23 06:46:46,962 epoch 20 - iter 12/43 - loss 4.00399901 - samples/sec: 82.10 - lr: 0.050000\n",
            "2020-12-23 06:46:48,732 epoch 20 - iter 16/43 - loss 4.01393966 - samples/sec: 72.63 - lr: 0.050000\n",
            "2020-12-23 06:46:50,344 epoch 20 - iter 20/43 - loss 3.92264074 - samples/sec: 79.44 - lr: 0.050000\n",
            "2020-12-23 06:46:52,066 epoch 20 - iter 24/43 - loss 3.90111713 - samples/sec: 74.46 - lr: 0.050000\n",
            "2020-12-23 06:46:53,523 epoch 20 - iter 28/43 - loss 3.86144350 - samples/sec: 87.92 - lr: 0.050000\n",
            "2020-12-23 06:46:55,062 epoch 20 - iter 32/43 - loss 3.78715202 - samples/sec: 83.22 - lr: 0.050000\n",
            "2020-12-23 06:46:56,655 epoch 20 - iter 36/43 - loss 3.74881343 - samples/sec: 80.43 - lr: 0.050000\n",
            "2020-12-23 06:46:58,181 epoch 20 - iter 40/43 - loss 3.67834262 - samples/sec: 83.93 - lr: 0.050000\n",
            "2020-12-23 06:46:59,255 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:46:59,256 EPOCH 20 done: loss 3.6193 - lr 0.0500000\n",
            "2020-12-23 06:47:00,294 DEV : loss 3.6830766201019287 - score 0.7139\n",
            "2020-12-23 06:47:00,301 BAD EPOCHS (no improvement): 3\n",
            "2020-12-23 06:47:00,302 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:47:01,808 epoch 21 - iter 4/43 - loss 3.84508520 - samples/sec: 85.21 - lr: 0.050000\n",
            "2020-12-23 06:47:03,267 epoch 21 - iter 8/43 - loss 3.49880269 - samples/sec: 87.84 - lr: 0.050000\n",
            "2020-12-23 06:47:04,800 epoch 21 - iter 12/43 - loss 3.40752751 - samples/sec: 83.55 - lr: 0.050000\n",
            "2020-12-23 06:47:06,486 epoch 21 - iter 16/43 - loss 3.26241408 - samples/sec: 75.99 - lr: 0.050000\n",
            "2020-12-23 06:47:08,128 epoch 21 - iter 20/43 - loss 3.41473981 - samples/sec: 78.03 - lr: 0.050000\n",
            "2020-12-23 06:47:09,665 epoch 21 - iter 24/43 - loss 3.57798075 - samples/sec: 83.33 - lr: 0.050000\n",
            "2020-12-23 06:47:11,203 epoch 21 - iter 28/43 - loss 3.49659288 - samples/sec: 83.29 - lr: 0.050000\n",
            "2020-12-23 06:47:12,866 epoch 21 - iter 32/43 - loss 3.54366855 - samples/sec: 77.14 - lr: 0.050000\n",
            "2020-12-23 06:47:14,500 epoch 21 - iter 36/43 - loss 3.46204885 - samples/sec: 78.39 - lr: 0.050000\n",
            "2020-12-23 06:47:16,093 epoch 21 - iter 40/43 - loss 3.54912768 - samples/sec: 80.40 - lr: 0.050000\n",
            "2020-12-23 06:47:17,245 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:47:17,250 EPOCH 21 done: loss 3.5870 - lr 0.0500000\n",
            "2020-12-23 06:47:18,287 DEV : loss 4.1208062171936035 - score 0.6968\n",
            "Epoch    21: reducing learning rate of group 0 to 2.5000e-02.\n",
            "2020-12-23 06:47:18,294 BAD EPOCHS (no improvement): 4\n",
            "2020-12-23 06:47:18,295 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:47:19,781 epoch 22 - iter 4/43 - loss 2.53266859 - samples/sec: 86.34 - lr: 0.025000\n",
            "2020-12-23 06:47:21,416 epoch 22 - iter 8/43 - loss 2.98554298 - samples/sec: 78.39 - lr: 0.025000\n",
            "2020-12-23 06:47:22,940 epoch 22 - iter 12/43 - loss 3.09518035 - samples/sec: 84.23 - lr: 0.025000\n",
            "2020-12-23 06:47:24,582 epoch 22 - iter 16/43 - loss 3.24250604 - samples/sec: 78.04 - lr: 0.025000\n",
            "2020-12-23 06:47:26,228 epoch 22 - iter 20/43 - loss 3.19468458 - samples/sec: 77.79 - lr: 0.025000\n",
            "2020-12-23 06:47:27,840 epoch 22 - iter 24/43 - loss 3.25471947 - samples/sec: 79.47 - lr: 0.025000\n",
            "2020-12-23 06:47:29,172 epoch 22 - iter 28/43 - loss 3.19678188 - samples/sec: 96.24 - lr: 0.025000\n",
            "2020-12-23 06:47:30,712 epoch 22 - iter 32/43 - loss 3.09364688 - samples/sec: 83.18 - lr: 0.025000\n",
            "2020-12-23 06:47:32,310 epoch 22 - iter 36/43 - loss 3.15508363 - samples/sec: 80.16 - lr: 0.025000\n",
            "2020-12-23 06:47:33,923 epoch 22 - iter 40/43 - loss 3.21444917 - samples/sec: 79.44 - lr: 0.025000\n",
            "2020-12-23 06:47:35,084 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:47:35,085 EPOCH 22 done: loss 3.1716 - lr 0.0250000\n",
            "2020-12-23 06:47:36,109 DEV : loss 3.6688966751098633 - score 0.7184\n",
            "2020-12-23 06:47:36,116 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:50:51,149 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:50:52,846 epoch 23 - iter 4/43 - loss 2.83262563 - samples/sec: 75.67 - lr: 0.025000\n",
            "2020-12-23 06:50:54,530 epoch 23 - iter 8/43 - loss 2.61108625 - samples/sec: 76.13 - lr: 0.025000\n",
            "2020-12-23 06:50:56,027 epoch 23 - iter 12/43 - loss 2.64930987 - samples/sec: 85.60 - lr: 0.025000\n",
            "2020-12-23 06:50:57,691 epoch 23 - iter 16/43 - loss 2.94860743 - samples/sec: 76.99 - lr: 0.025000\n",
            "2020-12-23 06:50:59,302 epoch 23 - iter 20/43 - loss 2.96816094 - samples/sec: 79.56 - lr: 0.025000\n",
            "2020-12-23 06:51:00,894 epoch 23 - iter 24/43 - loss 3.05619003 - samples/sec: 80.48 - lr: 0.025000\n",
            "2020-12-23 06:51:02,424 epoch 23 - iter 28/43 - loss 3.07102081 - samples/sec: 83.82 - lr: 0.025000\n",
            "2020-12-23 06:51:03,986 epoch 23 - iter 32/43 - loss 3.17832763 - samples/sec: 82.00 - lr: 0.025000\n",
            "2020-12-23 06:51:05,500 epoch 23 - iter 36/43 - loss 3.21929579 - samples/sec: 84.63 - lr: 0.025000\n",
            "2020-12-23 06:51:07,152 epoch 23 - iter 40/43 - loss 3.22045833 - samples/sec: 77.58 - lr: 0.025000\n",
            "2020-12-23 06:51:08,134 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:51:08,135 EPOCH 23 done: loss 3.2505 - lr 0.0250000\n",
            "2020-12-23 06:51:09,639 DEV : loss 3.964456796646118 - score 0.7111\n",
            "2020-12-23 06:51:09,647 BAD EPOCHS (no improvement): 1\n",
            "2020-12-23 06:51:09,649 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:51:11,186 epoch 24 - iter 4/43 - loss 2.48822683 - samples/sec: 83.44 - lr: 0.025000\n",
            "2020-12-23 06:51:12,806 epoch 24 - iter 8/43 - loss 2.63736680 - samples/sec: 79.12 - lr: 0.025000\n",
            "2020-12-23 06:51:14,442 epoch 24 - iter 12/43 - loss 3.06856408 - samples/sec: 78.30 - lr: 0.025000\n",
            "2020-12-23 06:51:16,070 epoch 24 - iter 16/43 - loss 3.15567128 - samples/sec: 78.67 - lr: 0.025000\n",
            "2020-12-23 06:51:17,657 epoch 24 - iter 20/43 - loss 3.18954743 - samples/sec: 80.73 - lr: 0.025000\n",
            "2020-12-23 06:51:19,164 epoch 24 - iter 24/43 - loss 3.25321544 - samples/sec: 85.04 - lr: 0.025000\n",
            "2020-12-23 06:51:20,555 epoch 24 - iter 28/43 - loss 3.15263288 - samples/sec: 92.34 - lr: 0.025000\n",
            "2020-12-23 06:51:22,211 epoch 24 - iter 32/43 - loss 3.15342640 - samples/sec: 77.43 - lr: 0.025000\n",
            "2020-12-23 06:51:23,654 epoch 24 - iter 36/43 - loss 3.08454027 - samples/sec: 88.84 - lr: 0.025000\n",
            "2020-12-23 06:51:25,258 epoch 24 - iter 40/43 - loss 3.09684284 - samples/sec: 79.91 - lr: 0.025000\n",
            "2020-12-23 06:51:26,276 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:51:26,277 EPOCH 24 done: loss 3.1075 - lr 0.0250000\n",
            "2020-12-23 06:51:27,307 DEV : loss 3.8440518379211426 - score 0.7233\n",
            "2020-12-23 06:51:27,314 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:51:50,614 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:51:53,586 epoch 25 - iter 4/43 - loss 2.47174299 - samples/sec: 78.06 - lr: 0.025000\n",
            "2020-12-23 06:51:55,247 epoch 25 - iter 8/43 - loss 2.66731173 - samples/sec: 77.25 - lr: 0.025000\n",
            "2020-12-23 06:51:56,910 epoch 25 - iter 12/43 - loss 2.76402851 - samples/sec: 77.07 - lr: 0.025000\n",
            "2020-12-23 06:51:58,448 epoch 25 - iter 16/43 - loss 2.89162329 - samples/sec: 83.29 - lr: 0.025000\n",
            "2020-12-23 06:52:00,009 epoch 25 - iter 20/43 - loss 2.82582974 - samples/sec: 82.08 - lr: 0.025000\n",
            "2020-12-23 06:52:01,531 epoch 25 - iter 24/43 - loss 2.96743577 - samples/sec: 84.18 - lr: 0.025000\n",
            "2020-12-23 06:52:03,084 epoch 25 - iter 28/43 - loss 3.05817389 - samples/sec: 82.47 - lr: 0.025000\n",
            "2020-12-23 06:52:04,664 epoch 25 - iter 32/43 - loss 3.04679371 - samples/sec: 81.39 - lr: 0.025000\n",
            "2020-12-23 06:52:06,186 epoch 25 - iter 36/43 - loss 3.04208371 - samples/sec: 84.17 - lr: 0.025000\n",
            "2020-12-23 06:52:07,713 epoch 25 - iter 40/43 - loss 3.05591771 - samples/sec: 83.87 - lr: 0.025000\n",
            "2020-12-23 06:52:08,778 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:52:08,780 EPOCH 25 done: loss 2.9779 - lr 0.0250000\n",
            "2020-12-23 06:52:09,790 DEV : loss 3.710570812225342 - score 0.7206\n",
            "2020-12-23 06:52:09,797 BAD EPOCHS (no improvement): 1\n",
            "2020-12-23 06:52:09,799 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:52:11,321 epoch 26 - iter 4/43 - loss 2.29606980 - samples/sec: 84.26 - lr: 0.025000\n",
            "2020-12-23 06:52:12,876 epoch 26 - iter 8/43 - loss 2.91097996 - samples/sec: 82.38 - lr: 0.025000\n",
            "2020-12-23 06:52:14,620 epoch 26 - iter 12/43 - loss 2.85394172 - samples/sec: 73.48 - lr: 0.025000\n",
            "2020-12-23 06:52:16,230 epoch 26 - iter 16/43 - loss 2.73995422 - samples/sec: 79.60 - lr: 0.025000\n",
            "2020-12-23 06:52:17,784 epoch 26 - iter 20/43 - loss 2.77509174 - samples/sec: 82.44 - lr: 0.025000\n",
            "2020-12-23 06:52:19,399 epoch 26 - iter 24/43 - loss 2.95677372 - samples/sec: 79.33 - lr: 0.025000\n",
            "2020-12-23 06:52:20,955 epoch 26 - iter 28/43 - loss 2.99834612 - samples/sec: 82.32 - lr: 0.025000\n",
            "2020-12-23 06:52:22,540 epoch 26 - iter 32/43 - loss 3.06743421 - samples/sec: 80.81 - lr: 0.025000\n",
            "2020-12-23 06:52:24,094 epoch 26 - iter 36/43 - loss 2.98993033 - samples/sec: 82.47 - lr: 0.025000\n",
            "2020-12-23 06:52:25,582 epoch 26 - iter 40/43 - loss 2.91674993 - samples/sec: 86.10 - lr: 0.025000\n",
            "2020-12-23 06:52:26,570 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:52:26,571 EPOCH 26 done: loss 2.9424 - lr 0.0250000\n",
            "2020-12-23 06:52:27,603 DEV : loss 3.776811361312866 - score 0.7235\n",
            "2020-12-23 06:52:27,610 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:55:21,124 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:55:22,932 epoch 27 - iter 4/43 - loss 3.64463091 - samples/sec: 70.97 - lr: 0.025000\n",
            "2020-12-23 06:55:24,482 epoch 27 - iter 8/43 - loss 3.56691504 - samples/sec: 82.67 - lr: 0.025000\n",
            "2020-12-23 06:55:26,148 epoch 27 - iter 12/43 - loss 3.33869358 - samples/sec: 76.91 - lr: 0.025000\n",
            "2020-12-23 06:55:27,730 epoch 27 - iter 16/43 - loss 3.49770303 - samples/sec: 80.99 - lr: 0.025000\n",
            "2020-12-23 06:55:29,405 epoch 27 - iter 20/43 - loss 3.22294437 - samples/sec: 76.49 - lr: 0.025000\n",
            "2020-12-23 06:55:31,048 epoch 27 - iter 24/43 - loss 3.23723914 - samples/sec: 77.97 - lr: 0.025000\n",
            "2020-12-23 06:55:32,534 epoch 27 - iter 28/43 - loss 3.09497097 - samples/sec: 86.26 - lr: 0.025000\n",
            "2020-12-23 06:55:34,094 epoch 27 - iter 32/43 - loss 3.02111289 - samples/sec: 82.12 - lr: 0.025000\n",
            "2020-12-23 06:55:35,601 epoch 27 - iter 36/43 - loss 2.99730498 - samples/sec: 85.02 - lr: 0.025000\n",
            "2020-12-23 06:55:37,095 epoch 27 - iter 40/43 - loss 2.92336504 - samples/sec: 85.75 - lr: 0.025000\n",
            "2020-12-23 06:55:38,061 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:55:38,063 EPOCH 27 done: loss 2.8815 - lr 0.0250000\n",
            "2020-12-23 06:55:39,081 DEV : loss 3.6381967067718506 - score 0.7294\n",
            "2020-12-23 06:55:39,087 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-12-23 06:56:02,214 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:56:05,877 epoch 28 - iter 4/43 - loss 2.16973072 - samples/sec: 80.96 - lr: 0.025000\n",
            "2020-12-23 06:56:07,460 epoch 28 - iter 8/43 - loss 2.45122492 - samples/sec: 80.94 - lr: 0.025000\n",
            "2020-12-23 06:56:09,010 epoch 28 - iter 12/43 - loss 2.48946883 - samples/sec: 82.64 - lr: 0.025000\n",
            "2020-12-23 06:56:10,555 epoch 28 - iter 16/43 - loss 2.69112228 - samples/sec: 82.91 - lr: 0.025000\n",
            "2020-12-23 06:56:11,979 epoch 28 - iter 20/43 - loss 2.66185455 - samples/sec: 89.98 - lr: 0.025000\n",
            "2020-12-23 06:56:13,617 epoch 28 - iter 24/43 - loss 2.89537173 - samples/sec: 78.22 - lr: 0.025000\n",
            "2020-12-23 06:56:15,133 epoch 28 - iter 28/43 - loss 2.94299465 - samples/sec: 84.49 - lr: 0.025000\n",
            "2020-12-23 06:56:16,724 epoch 28 - iter 32/43 - loss 2.93294246 - samples/sec: 80.54 - lr: 0.025000\n",
            "2020-12-23 06:56:18,465 epoch 28 - iter 36/43 - loss 2.87414119 - samples/sec: 73.56 - lr: 0.025000\n",
            "2020-12-23 06:56:20,026 epoch 28 - iter 40/43 - loss 2.80681564 - samples/sec: 82.08 - lr: 0.025000\n",
            "2020-12-23 06:56:21,144 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:56:21,147 EPOCH 28 done: loss 2.8892 - lr 0.0250000\n",
            "2020-12-23 06:56:22,191 DEV : loss 3.6773457527160645 - score 0.7244\n",
            "2020-12-23 06:56:22,198 BAD EPOCHS (no improvement): 1\n",
            "2020-12-23 06:56:22,201 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:56:23,687 epoch 29 - iter 4/43 - loss 2.81839800 - samples/sec: 86.30 - lr: 0.025000\n",
            "2020-12-23 06:56:25,212 epoch 29 - iter 8/43 - loss 2.70380926 - samples/sec: 84.05 - lr: 0.025000\n",
            "2020-12-23 06:56:26,816 epoch 29 - iter 12/43 - loss 2.71831393 - samples/sec: 79.96 - lr: 0.025000\n",
            "2020-12-23 06:56:28,434 epoch 29 - iter 16/43 - loss 2.72030391 - samples/sec: 79.19 - lr: 0.025000\n",
            "2020-12-23 06:56:30,175 epoch 29 - iter 20/43 - loss 2.73610845 - samples/sec: 73.60 - lr: 0.025000\n",
            "2020-12-23 06:56:31,720 epoch 29 - iter 24/43 - loss 2.82355287 - samples/sec: 82.93 - lr: 0.025000\n",
            "2020-12-23 06:56:33,263 epoch 29 - iter 28/43 - loss 2.79076530 - samples/sec: 83.00 - lr: 0.025000\n",
            "2020-12-23 06:56:34,690 epoch 29 - iter 32/43 - loss 2.73375871 - samples/sec: 89.81 - lr: 0.025000\n",
            "2020-12-23 06:56:36,407 epoch 29 - iter 36/43 - loss 2.76690586 - samples/sec: 74.81 - lr: 0.025000\n",
            "2020-12-23 06:56:37,980 epoch 29 - iter 40/43 - loss 2.72683113 - samples/sec: 81.50 - lr: 0.025000\n",
            "2020-12-23 06:56:39,136 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:56:39,139 EPOCH 29 done: loss 2.7376 - lr 0.0250000\n",
            "2020-12-23 06:56:40,149 DEV : loss 3.7301440238952637 - score 0.7255\n",
            "2020-12-23 06:56:40,156 BAD EPOCHS (no improvement): 2\n",
            "2020-12-23 06:56:40,157 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:56:41,634 epoch 30 - iter 4/43 - loss 2.95205891 - samples/sec: 86.84 - lr: 0.025000\n",
            "2020-12-23 06:56:43,158 epoch 30 - iter 8/43 - loss 3.21061206 - samples/sec: 84.09 - lr: 0.025000\n",
            "2020-12-23 06:56:44,889 epoch 30 - iter 12/43 - loss 3.22571460 - samples/sec: 74.04 - lr: 0.025000\n",
            "2020-12-23 06:56:46,486 epoch 30 - iter 16/43 - loss 2.93686056 - samples/sec: 80.23 - lr: 0.025000\n",
            "2020-12-23 06:56:48,008 epoch 30 - iter 20/43 - loss 2.97040565 - samples/sec: 84.19 - lr: 0.025000\n",
            "2020-12-23 06:56:49,570 epoch 30 - iter 24/43 - loss 2.95457672 - samples/sec: 82.03 - lr: 0.025000\n",
            "2020-12-23 06:56:51,317 epoch 30 - iter 28/43 - loss 2.91271626 - samples/sec: 73.34 - lr: 0.025000\n",
            "2020-12-23 06:56:52,852 epoch 30 - iter 32/43 - loss 2.82715437 - samples/sec: 83.43 - lr: 0.025000\n",
            "2020-12-23 06:56:54,448 epoch 30 - iter 36/43 - loss 2.84271324 - samples/sec: 80.37 - lr: 0.025000\n",
            "2020-12-23 06:56:56,182 epoch 30 - iter 40/43 - loss 2.83577427 - samples/sec: 73.86 - lr: 0.025000\n",
            "2020-12-23 06:56:57,223 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:56:57,225 EPOCH 30 done: loss 2.7821 - lr 0.0250000\n",
            "2020-12-23 06:56:58,261 DEV : loss 3.7477946281433105 - score 0.7207\n",
            "2020-12-23 06:56:58,268 BAD EPOCHS (no improvement): 3\n",
            "2020-12-23 06:56:58,269 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:56:59,740 epoch 31 - iter 4/43 - loss 1.77200806 - samples/sec: 87.20 - lr: 0.025000\n",
            "2020-12-23 06:57:01,256 epoch 31 - iter 8/43 - loss 2.29968747 - samples/sec: 84.45 - lr: 0.025000\n",
            "2020-12-23 06:57:02,689 epoch 31 - iter 12/43 - loss 2.32245086 - samples/sec: 89.44 - lr: 0.025000\n",
            "2020-12-23 06:57:04,259 epoch 31 - iter 16/43 - loss 2.24621676 - samples/sec: 81.63 - lr: 0.025000\n",
            "2020-12-23 06:57:05,901 epoch 31 - iter 20/43 - loss 2.28095134 - samples/sec: 78.02 - lr: 0.025000\n",
            "2020-12-23 06:57:07,457 epoch 31 - iter 24/43 - loss 2.32891867 - samples/sec: 82.35 - lr: 0.025000\n",
            "2020-12-23 06:57:09,223 epoch 31 - iter 28/43 - loss 2.48377900 - samples/sec: 72.51 - lr: 0.025000\n",
            "2020-12-23 06:57:10,873 epoch 31 - iter 32/43 - loss 2.57869819 - samples/sec: 77.63 - lr: 0.025000\n",
            "2020-12-23 06:57:12,440 epoch 31 - iter 36/43 - loss 2.70483807 - samples/sec: 81.79 - lr: 0.025000\n",
            "2020-12-23 06:57:13,815 epoch 31 - iter 40/43 - loss 2.69028327 - samples/sec: 93.14 - lr: 0.025000\n",
            "2020-12-23 06:57:14,904 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:57:14,905 EPOCH 31 done: loss 2.7324 - lr 0.0250000\n",
            "2020-12-23 06:57:15,945 DEV : loss 3.6717283725738525 - score 0.7108\n",
            "Epoch    31: reducing learning rate of group 0 to 1.2500e-02.\n",
            "2020-12-23 06:57:15,952 BAD EPOCHS (no improvement): 4\n",
            "2020-12-23 06:57:15,953 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:57:17,496 epoch 32 - iter 4/43 - loss 2.66322869 - samples/sec: 83.12 - lr: 0.012500\n",
            "2020-12-23 06:57:19,068 epoch 32 - iter 8/43 - loss 2.76398534 - samples/sec: 81.57 - lr: 0.012500\n",
            "2020-12-23 06:57:20,625 epoch 32 - iter 12/43 - loss 2.63497039 - samples/sec: 82.53 - lr: 0.012500\n",
            "2020-12-23 06:57:22,164 epoch 32 - iter 16/43 - loss 2.56657881 - samples/sec: 83.23 - lr: 0.012500\n",
            "2020-12-23 06:57:23,772 epoch 32 - iter 20/43 - loss 2.58054880 - samples/sec: 79.67 - lr: 0.012500\n",
            "2020-12-23 06:57:25,319 epoch 32 - iter 24/43 - loss 2.51302769 - samples/sec: 82.84 - lr: 0.012500\n",
            "2020-12-23 06:57:26,861 epoch 32 - iter 28/43 - loss 2.48202237 - samples/sec: 83.09 - lr: 0.012500\n",
            "2020-12-23 06:57:28,465 epoch 32 - iter 32/43 - loss 2.56038638 - samples/sec: 79.85 - lr: 0.012500\n",
            "2020-12-23 06:57:30,010 epoch 32 - iter 36/43 - loss 2.50978440 - samples/sec: 82.95 - lr: 0.012500\n",
            "2020-12-23 06:57:31,747 epoch 32 - iter 40/43 - loss 2.56982844 - samples/sec: 73.75 - lr: 0.012500\n",
            "2020-12-23 06:57:32,804 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:57:32,807 EPOCH 32 done: loss 2.6197 - lr 0.0125000\n",
            "2020-12-23 06:57:33,845 DEV : loss 3.706446409225464 - score 0.7178\n",
            "2020-12-23 06:57:33,852 BAD EPOCHS (no improvement): 1\n",
            "2020-12-23 06:57:33,853 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:57:35,373 epoch 33 - iter 4/43 - loss 1.61235517 - samples/sec: 84.35 - lr: 0.012500\n",
            "2020-12-23 06:57:36,979 epoch 33 - iter 8/43 - loss 2.30957076 - samples/sec: 79.85 - lr: 0.012500\n",
            "2020-12-23 06:57:38,521 epoch 33 - iter 12/43 - loss 2.77641420 - samples/sec: 83.06 - lr: 0.012500\n",
            "2020-12-23 06:57:40,107 epoch 33 - iter 16/43 - loss 2.79592079 - samples/sec: 80.88 - lr: 0.012500\n",
            "2020-12-23 06:57:41,653 epoch 33 - iter 20/43 - loss 2.70350009 - samples/sec: 82.85 - lr: 0.012500\n",
            "2020-12-23 06:57:43,168 epoch 33 - iter 24/43 - loss 2.64153833 - samples/sec: 84.57 - lr: 0.012500\n",
            "2020-12-23 06:57:44,801 epoch 33 - iter 28/43 - loss 2.67948158 - samples/sec: 78.44 - lr: 0.012500\n",
            "2020-12-23 06:57:46,426 epoch 33 - iter 32/43 - loss 2.64164633 - samples/sec: 78.80 - lr: 0.012500\n",
            "2020-12-23 06:57:48,061 epoch 33 - iter 36/43 - loss 2.66339817 - samples/sec: 78.37 - lr: 0.012500\n",
            "2020-12-23 06:57:49,649 epoch 33 - iter 40/43 - loss 2.59970585 - samples/sec: 80.88 - lr: 0.012500\n",
            "2020-12-23 06:57:50,693 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:57:50,694 EPOCH 33 done: loss 2.6398 - lr 0.0125000\n",
            "2020-12-23 06:57:51,718 DEV : loss 3.7481448650360107 - score 0.7065\n",
            "2020-12-23 06:57:51,722 BAD EPOCHS (no improvement): 2\n",
            "2020-12-23 06:57:51,723 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:57:53,288 epoch 34 - iter 4/43 - loss 2.85605407 - samples/sec: 82.00 - lr: 0.012500\n",
            "2020-12-23 06:57:54,894 epoch 34 - iter 8/43 - loss 2.58063331 - samples/sec: 79.75 - lr: 0.012500\n",
            "2020-12-23 06:57:56,357 epoch 34 - iter 12/43 - loss 2.60852466 - samples/sec: 87.57 - lr: 0.012500\n",
            "2020-12-23 06:57:57,886 epoch 34 - iter 16/43 - loss 2.49845366 - samples/sec: 83.79 - lr: 0.012500\n",
            "2020-12-23 06:57:59,482 epoch 34 - iter 20/43 - loss 2.44292632 - samples/sec: 80.24 - lr: 0.012500\n",
            "2020-12-23 06:58:00,998 epoch 34 - iter 24/43 - loss 2.51351730 - samples/sec: 84.53 - lr: 0.012500\n",
            "2020-12-23 06:58:02,607 epoch 34 - iter 28/43 - loss 2.61606379 - samples/sec: 79.64 - lr: 0.012500\n",
            "2020-12-23 06:58:04,301 epoch 34 - iter 32/43 - loss 2.66432150 - samples/sec: 75.60 - lr: 0.012500\n",
            "2020-12-23 06:58:05,944 epoch 34 - iter 36/43 - loss 2.64859233 - samples/sec: 77.97 - lr: 0.012500\n",
            "2020-12-23 06:58:07,487 epoch 34 - iter 40/43 - loss 2.62549866 - samples/sec: 83.03 - lr: 0.012500\n",
            "2020-12-23 06:58:08,537 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:58:08,539 EPOCH 34 done: loss 2.5451 - lr 0.0125000\n",
            "2020-12-23 06:58:09,551 DEV : loss 3.5523171424865723 - score 0.7286\n",
            "2020-12-23 06:58:09,559 BAD EPOCHS (no improvement): 3\n",
            "2020-12-23 06:58:09,560 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:58:11,195 epoch 35 - iter 4/43 - loss 2.97102404 - samples/sec: 78.45 - lr: 0.012500\n",
            "2020-12-23 06:58:12,678 epoch 35 - iter 8/43 - loss 2.66711491 - samples/sec: 86.40 - lr: 0.012500\n",
            "2020-12-23 06:58:14,174 epoch 35 - iter 12/43 - loss 2.42700293 - samples/sec: 85.65 - lr: 0.012500\n",
            "2020-12-23 06:58:15,774 epoch 35 - iter 16/43 - loss 2.39910604 - samples/sec: 80.06 - lr: 0.012500\n",
            "2020-12-23 06:58:17,391 epoch 35 - iter 20/43 - loss 2.61148832 - samples/sec: 79.21 - lr: 0.012500\n",
            "2020-12-23 06:58:18,875 epoch 35 - iter 24/43 - loss 2.56855918 - samples/sec: 86.36 - lr: 0.012500\n",
            "2020-12-23 06:58:20,465 epoch 35 - iter 28/43 - loss 2.51810690 - samples/sec: 80.57 - lr: 0.012500\n",
            "2020-12-23 06:58:22,009 epoch 35 - iter 32/43 - loss 2.45400911 - samples/sec: 82.99 - lr: 0.012500\n",
            "2020-12-23 06:58:23,563 epoch 35 - iter 36/43 - loss 2.42210068 - samples/sec: 82.47 - lr: 0.012500\n",
            "2020-12-23 06:58:25,299 epoch 35 - iter 40/43 - loss 2.50476673 - samples/sec: 73.78 - lr: 0.012500\n",
            "2020-12-23 06:58:26,515 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:58:26,516 EPOCH 35 done: loss 2.4685 - lr 0.0125000\n",
            "2020-12-23 06:58:27,555 DEV : loss 3.80267333984375 - score 0.7103\n",
            "Epoch    35: reducing learning rate of group 0 to 6.2500e-03.\n",
            "2020-12-23 06:58:27,561 BAD EPOCHS (no improvement): 4\n",
            "2020-12-23 06:58:27,563 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:58:29,202 epoch 36 - iter 4/43 - loss 2.61857247 - samples/sec: 78.20 - lr: 0.006250\n",
            "2020-12-23 06:58:30,758 epoch 36 - iter 8/43 - loss 2.46360299 - samples/sec: 82.36 - lr: 0.006250\n",
            "2020-12-23 06:58:32,412 epoch 36 - iter 12/43 - loss 2.70355294 - samples/sec: 77.43 - lr: 0.006250\n",
            "2020-12-23 06:58:34,093 epoch 36 - iter 16/43 - loss 2.54780318 - samples/sec: 76.25 - lr: 0.006250\n",
            "2020-12-23 06:58:35,665 epoch 36 - iter 20/43 - loss 2.45724986 - samples/sec: 81.45 - lr: 0.006250\n",
            "2020-12-23 06:58:37,117 epoch 36 - iter 24/43 - loss 2.46930020 - samples/sec: 88.23 - lr: 0.006250\n",
            "2020-12-23 06:58:38,608 epoch 36 - iter 28/43 - loss 2.49851365 - samples/sec: 86.12 - lr: 0.006250\n",
            "2020-12-23 06:58:40,168 epoch 36 - iter 32/43 - loss 2.45385595 - samples/sec: 82.12 - lr: 0.006250\n",
            "2020-12-23 06:58:41,783 epoch 36 - iter 36/43 - loss 2.48842554 - samples/sec: 79.32 - lr: 0.006250\n",
            "2020-12-23 06:58:43,454 epoch 36 - iter 40/43 - loss 2.50416965 - samples/sec: 76.66 - lr: 0.006250\n",
            "2020-12-23 06:58:44,412 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:58:44,413 EPOCH 36 done: loss 2.4444 - lr 0.0062500\n",
            "2020-12-23 06:58:45,435 DEV : loss 3.768207550048828 - score 0.715\n",
            "2020-12-23 06:58:45,442 BAD EPOCHS (no improvement): 1\n",
            "2020-12-23 06:58:45,446 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:58:46,987 epoch 37 - iter 4/43 - loss 1.98187524 - samples/sec: 83.21 - lr: 0.006250\n",
            "2020-12-23 06:58:48,539 epoch 37 - iter 8/43 - loss 2.48083830 - samples/sec: 82.57 - lr: 0.006250\n",
            "2020-12-23 06:58:50,215 epoch 37 - iter 12/43 - loss 2.35043931 - samples/sec: 76.50 - lr: 0.006250\n",
            "2020-12-23 06:58:51,875 epoch 37 - iter 16/43 - loss 2.55656093 - samples/sec: 77.18 - lr: 0.006250\n",
            "2020-12-23 06:58:53,454 epoch 37 - iter 20/43 - loss 2.44534117 - samples/sec: 81.18 - lr: 0.006250\n",
            "2020-12-23 06:58:55,046 epoch 37 - iter 24/43 - loss 2.42462524 - samples/sec: 80.46 - lr: 0.006250\n",
            "2020-12-23 06:58:56,528 epoch 37 - iter 28/43 - loss 2.31799814 - samples/sec: 86.44 - lr: 0.006250\n",
            "2020-12-23 06:58:58,181 epoch 37 - iter 32/43 - loss 2.26924794 - samples/sec: 77.50 - lr: 0.006250\n",
            "2020-12-23 06:58:59,660 epoch 37 - iter 36/43 - loss 2.32065468 - samples/sec: 86.62 - lr: 0.006250\n",
            "2020-12-23 06:59:01,385 epoch 37 - iter 40/43 - loss 2.41446127 - samples/sec: 74.35 - lr: 0.006250\n",
            "2020-12-23 06:59:02,317 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:59:02,318 EPOCH 37 done: loss 2.3573 - lr 0.0062500\n",
            "2020-12-23 06:59:03,344 DEV : loss 3.7337565422058105 - score 0.7119\n",
            "2020-12-23 06:59:03,352 BAD EPOCHS (no improvement): 2\n",
            "2020-12-23 06:59:03,353 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:59:04,884 epoch 38 - iter 4/43 - loss 2.20180929 - samples/sec: 83.79 - lr: 0.006250\n",
            "2020-12-23 06:59:06,482 epoch 38 - iter 8/43 - loss 2.30104247 - samples/sec: 80.18 - lr: 0.006250\n",
            "2020-12-23 06:59:08,056 epoch 38 - iter 12/43 - loss 2.31622914 - samples/sec: 81.39 - lr: 0.006250\n",
            "2020-12-23 06:59:09,692 epoch 38 - iter 16/43 - loss 2.32621861 - samples/sec: 78.31 - lr: 0.006250\n",
            "2020-12-23 06:59:11,184 epoch 38 - iter 20/43 - loss 2.25210345 - samples/sec: 85.86 - lr: 0.006250\n",
            "2020-12-23 06:59:12,854 epoch 38 - iter 24/43 - loss 2.35296399 - samples/sec: 76.71 - lr: 0.006250\n",
            "2020-12-23 06:59:14,360 epoch 38 - iter 28/43 - loss 2.40332043 - samples/sec: 85.09 - lr: 0.006250\n",
            "2020-12-23 06:59:15,842 epoch 38 - iter 32/43 - loss 2.32559139 - samples/sec: 86.44 - lr: 0.006250\n",
            "2020-12-23 06:59:17,393 epoch 38 - iter 36/43 - loss 2.37938341 - samples/sec: 82.58 - lr: 0.006250\n",
            "2020-12-23 06:59:19,050 epoch 38 - iter 40/43 - loss 2.38277639 - samples/sec: 77.43 - lr: 0.006250\n",
            "2020-12-23 06:59:20,049 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:59:20,050 EPOCH 38 done: loss 2.4725 - lr 0.0062500\n",
            "2020-12-23 06:59:21,077 DEV : loss 3.701817035675049 - score 0.7171\n",
            "2020-12-23 06:59:21,084 BAD EPOCHS (no improvement): 3\n",
            "2020-12-23 06:59:21,085 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:59:22,646 epoch 39 - iter 4/43 - loss 2.06888324 - samples/sec: 82.13 - lr: 0.006250\n",
            "2020-12-23 06:59:24,267 epoch 39 - iter 8/43 - loss 2.29655755 - samples/sec: 79.06 - lr: 0.006250\n",
            "2020-12-23 06:59:25,871 epoch 39 - iter 12/43 - loss 2.26791330 - samples/sec: 79.84 - lr: 0.006250\n",
            "2020-12-23 06:59:27,374 epoch 39 - iter 16/43 - loss 2.15851822 - samples/sec: 85.26 - lr: 0.006250\n",
            "2020-12-23 06:59:29,073 epoch 39 - iter 20/43 - loss 2.18594674 - samples/sec: 75.44 - lr: 0.006250\n",
            "2020-12-23 06:59:30,511 epoch 39 - iter 24/43 - loss 2.24374974 - samples/sec: 89.14 - lr: 0.006250\n",
            "2020-12-23 06:59:32,126 epoch 39 - iter 28/43 - loss 2.33886344 - samples/sec: 79.33 - lr: 0.006250\n",
            "2020-12-23 06:59:33,648 epoch 39 - iter 32/43 - loss 2.42012544 - samples/sec: 84.14 - lr: 0.006250\n",
            "2020-12-23 06:59:35,307 epoch 39 - iter 36/43 - loss 2.39779931 - samples/sec: 77.24 - lr: 0.006250\n",
            "2020-12-23 06:59:37,003 epoch 39 - iter 40/43 - loss 2.45276521 - samples/sec: 75.60 - lr: 0.006250\n",
            "2020-12-23 06:59:38,084 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:59:38,085 EPOCH 39 done: loss 2.4056 - lr 0.0062500\n",
            "2020-12-23 06:59:39,160 DEV : loss 3.700406789779663 - score 0.7199\n",
            "Epoch    39: reducing learning rate of group 0 to 3.1250e-03.\n",
            "2020-12-23 06:59:39,168 BAD EPOCHS (no improvement): 4\n",
            "2020-12-23 06:59:39,170 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:59:40,909 epoch 40 - iter 4/43 - loss 2.31790584 - samples/sec: 73.74 - lr: 0.003125\n",
            "2020-12-23 06:59:42,503 epoch 40 - iter 8/43 - loss 2.00526661 - samples/sec: 80.41 - lr: 0.003125\n",
            "2020-12-23 06:59:44,149 epoch 40 - iter 12/43 - loss 2.25093327 - samples/sec: 77.81 - lr: 0.003125\n",
            "2020-12-23 06:59:45,778 epoch 40 - iter 16/43 - loss 2.39592776 - samples/sec: 78.72 - lr: 0.003125\n",
            "2020-12-23 06:59:47,430 epoch 40 - iter 20/43 - loss 2.42767341 - samples/sec: 77.63 - lr: 0.003125\n",
            "2020-12-23 06:59:48,990 epoch 40 - iter 24/43 - loss 2.34505690 - samples/sec: 82.08 - lr: 0.003125\n",
            "2020-12-23 06:59:50,636 epoch 40 - iter 28/43 - loss 2.46719799 - samples/sec: 78.00 - lr: 0.003125\n",
            "2020-12-23 06:59:52,147 epoch 40 - iter 32/43 - loss 2.46381224 - samples/sec: 84.80 - lr: 0.003125\n",
            "2020-12-23 06:59:53,705 epoch 40 - iter 36/43 - loss 2.40877347 - samples/sec: 82.24 - lr: 0.003125\n",
            "2020-12-23 06:59:55,319 epoch 40 - iter 40/43 - loss 2.41948709 - samples/sec: 79.37 - lr: 0.003125\n",
            "2020-12-23 06:59:56,441 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:59:56,442 EPOCH 40 done: loss 2.3718 - lr 0.0031250\n",
            "2020-12-23 06:59:58,030 DEV : loss 3.7018449306488037 - score 0.7264\n",
            "2020-12-23 06:59:58,037 BAD EPOCHS (no improvement): 1\n",
            "2020-12-23 06:59:58,040 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 06:59:59,605 epoch 41 - iter 4/43 - loss 2.38338810 - samples/sec: 81.99 - lr: 0.003125\n",
            "2020-12-23 07:00:01,032 epoch 41 - iter 8/43 - loss 1.95385173 - samples/sec: 89.73 - lr: 0.003125\n",
            "2020-12-23 07:00:02,543 epoch 41 - iter 12/43 - loss 2.37176035 - samples/sec: 84.80 - lr: 0.003125\n",
            "2020-12-23 07:00:04,141 epoch 41 - iter 16/43 - loss 2.42060934 - samples/sec: 80.16 - lr: 0.003125\n",
            "2020-12-23 07:00:05,745 epoch 41 - iter 20/43 - loss 2.44609725 - samples/sec: 79.90 - lr: 0.003125\n",
            "2020-12-23 07:00:07,490 epoch 41 - iter 24/43 - loss 2.34468545 - samples/sec: 73.45 - lr: 0.003125\n",
            "2020-12-23 07:00:09,109 epoch 41 - iter 28/43 - loss 2.22623333 - samples/sec: 79.12 - lr: 0.003125\n",
            "2020-12-23 07:00:10,823 epoch 41 - iter 32/43 - loss 2.37970410 - samples/sec: 74.73 - lr: 0.003125\n",
            "2020-12-23 07:00:12,519 epoch 41 - iter 36/43 - loss 2.38104830 - samples/sec: 75.56 - lr: 0.003125\n",
            "2020-12-23 07:00:14,155 epoch 41 - iter 40/43 - loss 2.43486633 - samples/sec: 78.27 - lr: 0.003125\n",
            "2020-12-23 07:00:15,221 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:00:15,222 EPOCH 41 done: loss 2.3830 - lr 0.0031250\n",
            "2020-12-23 07:00:16,281 DEV : loss 3.700836658477783 - score 0.7215\n",
            "2020-12-23 07:00:16,285 BAD EPOCHS (no improvement): 2\n",
            "2020-12-23 07:00:16,287 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:00:17,890 epoch 42 - iter 4/43 - loss 2.32619739 - samples/sec: 79.96 - lr: 0.003125\n",
            "2020-12-23 07:00:19,475 epoch 42 - iter 8/43 - loss 2.29153591 - samples/sec: 80.84 - lr: 0.003125\n",
            "2020-12-23 07:00:21,149 epoch 42 - iter 12/43 - loss 2.27974103 - samples/sec: 76.51 - lr: 0.003125\n",
            "2020-12-23 07:00:22,657 epoch 42 - iter 16/43 - loss 2.35674667 - samples/sec: 84.98 - lr: 0.003125\n",
            "2020-12-23 07:00:24,230 epoch 42 - iter 20/43 - loss 2.33794322 - samples/sec: 81.45 - lr: 0.003125\n",
            "2020-12-23 07:00:25,795 epoch 42 - iter 24/43 - loss 2.46118219 - samples/sec: 81.85 - lr: 0.003125\n",
            "2020-12-23 07:00:27,163 epoch 42 - iter 28/43 - loss 2.39703465 - samples/sec: 93.67 - lr: 0.003125\n",
            "2020-12-23 07:00:28,751 epoch 42 - iter 32/43 - loss 2.35026435 - samples/sec: 80.77 - lr: 0.003125\n",
            "2020-12-23 07:00:30,359 epoch 42 - iter 36/43 - loss 2.32103138 - samples/sec: 79.68 - lr: 0.003125\n",
            "2020-12-23 07:00:31,930 epoch 42 - iter 40/43 - loss 2.29426039 - samples/sec: 81.50 - lr: 0.003125\n",
            "2020-12-23 07:00:33,059 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:00:33,060 EPOCH 42 done: loss 2.3196 - lr 0.0031250\n",
            "2020-12-23 07:00:34,084 DEV : loss 3.7391464710235596 - score 0.7195\n",
            "2020-12-23 07:00:34,094 BAD EPOCHS (no improvement): 3\n",
            "2020-12-23 07:00:34,095 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:00:35,593 epoch 43 - iter 4/43 - loss 1.82059973 - samples/sec: 85.64 - lr: 0.003125\n",
            "2020-12-23 07:00:37,245 epoch 43 - iter 8/43 - loss 2.48632604 - samples/sec: 77.53 - lr: 0.003125\n",
            "2020-12-23 07:00:38,851 epoch 43 - iter 12/43 - loss 2.39676247 - samples/sec: 79.80 - lr: 0.003125\n",
            "2020-12-23 07:00:40,438 epoch 43 - iter 16/43 - loss 2.47250931 - samples/sec: 80.72 - lr: 0.003125\n",
            "2020-12-23 07:00:41,988 epoch 43 - iter 20/43 - loss 2.36500242 - samples/sec: 82.69 - lr: 0.003125\n",
            "2020-12-23 07:00:43,557 epoch 43 - iter 24/43 - loss 2.26928069 - samples/sec: 81.63 - lr: 0.003125\n",
            "2020-12-23 07:00:45,108 epoch 43 - iter 28/43 - loss 2.36705203 - samples/sec: 82.60 - lr: 0.003125\n",
            "2020-12-23 07:00:46,778 epoch 43 - iter 32/43 - loss 2.31220353 - samples/sec: 76.70 - lr: 0.003125\n",
            "2020-12-23 07:00:48,360 epoch 43 - iter 36/43 - loss 2.31029113 - samples/sec: 80.97 - lr: 0.003125\n",
            "2020-12-23 07:00:49,951 epoch 43 - iter 40/43 - loss 2.30916085 - samples/sec: 80.56 - lr: 0.003125\n",
            "2020-12-23 07:00:50,864 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:00:50,867 EPOCH 43 done: loss 2.3105 - lr 0.0031250\n",
            "2020-12-23 07:00:51,886 DEV : loss 3.683899402618408 - score 0.7183\n",
            "Epoch    43: reducing learning rate of group 0 to 1.5625e-03.\n",
            "2020-12-23 07:00:51,895 BAD EPOCHS (no improvement): 4\n",
            "2020-12-23 07:00:51,898 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:00:53,492 epoch 44 - iter 4/43 - loss 2.45120555 - samples/sec: 80.51 - lr: 0.001563\n",
            "2020-12-23 07:00:55,063 epoch 44 - iter 8/43 - loss 2.16827044 - samples/sec: 81.55 - lr: 0.001563\n",
            "2020-12-23 07:00:56,648 epoch 44 - iter 12/43 - loss 2.59224004 - samples/sec: 80.79 - lr: 0.001563\n",
            "2020-12-23 07:00:58,312 epoch 44 - iter 16/43 - loss 2.39636818 - samples/sec: 76.97 - lr: 0.001563\n",
            "2020-12-23 07:00:59,978 epoch 44 - iter 20/43 - loss 2.25377808 - samples/sec: 76.90 - lr: 0.001563\n",
            "2020-12-23 07:01:01,602 epoch 44 - iter 24/43 - loss 2.42540099 - samples/sec: 78.95 - lr: 0.001563\n",
            "2020-12-23 07:01:03,291 epoch 44 - iter 28/43 - loss 2.42901903 - samples/sec: 75.93 - lr: 0.001563\n",
            "2020-12-23 07:01:04,801 epoch 44 - iter 32/43 - loss 2.44045445 - samples/sec: 85.00 - lr: 0.001563\n",
            "2020-12-23 07:01:06,368 epoch 44 - iter 36/43 - loss 2.40247709 - samples/sec: 81.77 - lr: 0.001563\n",
            "2020-12-23 07:01:07,889 epoch 44 - iter 40/43 - loss 2.34949361 - samples/sec: 84.22 - lr: 0.001563\n",
            "2020-12-23 07:01:08,810 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:01:08,811 EPOCH 44 done: loss 2.3997 - lr 0.0015625\n",
            "2020-12-23 07:01:09,815 DEV : loss 3.7267754077911377 - score 0.7146\n",
            "2020-12-23 07:01:09,821 BAD EPOCHS (no improvement): 1\n",
            "2020-12-23 07:01:09,822 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:01:11,487 epoch 45 - iter 4/43 - loss 2.59546328 - samples/sec: 77.04 - lr: 0.001563\n",
            "2020-12-23 07:01:13,063 epoch 45 - iter 8/43 - loss 2.80489630 - samples/sec: 81.38 - lr: 0.001563\n",
            "2020-12-23 07:01:14,532 epoch 45 - iter 12/43 - loss 2.39669903 - samples/sec: 87.25 - lr: 0.001563\n",
            "2020-12-23 07:01:16,209 epoch 45 - iter 16/43 - loss 2.56192847 - samples/sec: 76.37 - lr: 0.001563\n",
            "2020-12-23 07:01:17,845 epoch 45 - iter 20/43 - loss 2.44517493 - samples/sec: 78.32 - lr: 0.001563\n",
            "2020-12-23 07:01:19,439 epoch 45 - iter 24/43 - loss 2.47399062 - samples/sec: 80.33 - lr: 0.001563\n",
            "2020-12-23 07:01:20,963 epoch 45 - iter 28/43 - loss 2.50875289 - samples/sec: 84.08 - lr: 0.001563\n",
            "2020-12-23 07:01:22,529 epoch 45 - iter 32/43 - loss 2.42939737 - samples/sec: 81.81 - lr: 0.001563\n",
            "2020-12-23 07:01:24,063 epoch 45 - iter 36/43 - loss 2.39169502 - samples/sec: 83.49 - lr: 0.001563\n",
            "2020-12-23 07:01:25,614 epoch 45 - iter 40/43 - loss 2.33709515 - samples/sec: 82.61 - lr: 0.001563\n",
            "2020-12-23 07:01:26,715 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:01:26,716 EPOCH 45 done: loss 2.2986 - lr 0.0015625\n",
            "2020-12-23 07:01:27,808 DEV : loss 3.747907876968384 - score 0.7195\n",
            "2020-12-23 07:01:27,815 BAD EPOCHS (no improvement): 2\n",
            "2020-12-23 07:01:27,816 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:01:29,149 epoch 46 - iter 4/43 - loss 2.20050460 - samples/sec: 96.27 - lr: 0.001563\n",
            "2020-12-23 07:01:30,765 epoch 46 - iter 8/43 - loss 2.35608351 - samples/sec: 79.26 - lr: 0.001563\n",
            "2020-12-23 07:01:32,312 epoch 46 - iter 12/43 - loss 2.09253647 - samples/sec: 82.88 - lr: 0.001563\n",
            "2020-12-23 07:01:33,839 epoch 46 - iter 16/43 - loss 2.27979730 - samples/sec: 83.94 - lr: 0.001563\n",
            "2020-12-23 07:01:35,482 epoch 46 - iter 20/43 - loss 2.29904321 - samples/sec: 77.95 - lr: 0.001563\n",
            "2020-12-23 07:01:36,963 epoch 46 - iter 24/43 - loss 2.13545598 - samples/sec: 86.50 - lr: 0.001563\n",
            "2020-12-23 07:01:38,607 epoch 46 - iter 28/43 - loss 2.23370777 - samples/sec: 77.91 - lr: 0.001563\n",
            "2020-12-23 07:01:40,099 epoch 46 - iter 32/43 - loss 2.20905620 - samples/sec: 85.94 - lr: 0.001563\n",
            "2020-12-23 07:01:41,705 epoch 46 - iter 36/43 - loss 2.38093997 - samples/sec: 79.77 - lr: 0.001563\n",
            "2020-12-23 07:01:43,377 epoch 46 - iter 40/43 - loss 2.36746443 - samples/sec: 76.62 - lr: 0.001563\n",
            "2020-12-23 07:01:44,492 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:01:44,494 EPOCH 46 done: loss 2.3704 - lr 0.0015625\n",
            "2020-12-23 07:01:45,503 DEV : loss 3.712575674057007 - score 0.7173\n",
            "2020-12-23 07:01:45,507 BAD EPOCHS (no improvement): 3\n",
            "2020-12-23 07:01:45,508 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:01:47,001 epoch 47 - iter 4/43 - loss 2.09818953 - samples/sec: 85.91 - lr: 0.001563\n",
            "2020-12-23 07:01:48,530 epoch 47 - iter 8/43 - loss 2.19818085 - samples/sec: 83.83 - lr: 0.001563\n",
            "2020-12-23 07:01:50,178 epoch 47 - iter 12/43 - loss 2.24262275 - samples/sec: 77.73 - lr: 0.001563\n",
            "2020-12-23 07:01:51,755 epoch 47 - iter 16/43 - loss 2.28512380 - samples/sec: 81.24 - lr: 0.001563\n",
            "2020-12-23 07:01:53,281 epoch 47 - iter 20/43 - loss 2.28433266 - samples/sec: 84.00 - lr: 0.001563\n",
            "2020-12-23 07:01:54,908 epoch 47 - iter 24/43 - loss 2.32641083 - samples/sec: 78.75 - lr: 0.001563\n",
            "2020-12-23 07:01:56,705 epoch 47 - iter 28/43 - loss 2.24612738 - samples/sec: 71.30 - lr: 0.001563\n",
            "2020-12-23 07:01:58,268 epoch 47 - iter 32/43 - loss 2.34949594 - samples/sec: 81.95 - lr: 0.001563\n",
            "2020-12-23 07:01:59,752 epoch 47 - iter 36/43 - loss 2.32753337 - samples/sec: 86.37 - lr: 0.001563\n",
            "2020-12-23 07:02:01,187 epoch 47 - iter 40/43 - loss 2.33557666 - samples/sec: 89.25 - lr: 0.001563\n",
            "2020-12-23 07:02:02,241 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:02:02,242 EPOCH 47 done: loss 2.2600 - lr 0.0015625\n",
            "2020-12-23 07:02:03,244 DEV : loss 3.726006031036377 - score 0.7173\n",
            "Epoch    47: reducing learning rate of group 0 to 7.8125e-04.\n",
            "2020-12-23 07:02:03,250 BAD EPOCHS (no improvement): 4\n",
            "2020-12-23 07:02:03,251 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:02:04,685 epoch 48 - iter 4/43 - loss 2.51150149 - samples/sec: 89.45 - lr: 0.000781\n",
            "2020-12-23 07:02:06,248 epoch 48 - iter 8/43 - loss 2.72831276 - samples/sec: 82.00 - lr: 0.000781\n",
            "2020-12-23 07:02:07,831 epoch 48 - iter 12/43 - loss 2.62460931 - samples/sec: 80.95 - lr: 0.000781\n",
            "2020-12-23 07:02:09,351 epoch 48 - iter 16/43 - loss 2.40306698 - samples/sec: 84.28 - lr: 0.000781\n",
            "2020-12-23 07:02:11,104 epoch 48 - iter 20/43 - loss 2.35226598 - samples/sec: 73.08 - lr: 0.000781\n",
            "2020-12-23 07:02:12,689 epoch 48 - iter 24/43 - loss 2.45667424 - samples/sec: 80.91 - lr: 0.000781\n",
            "2020-12-23 07:02:14,298 epoch 48 - iter 28/43 - loss 2.38737391 - samples/sec: 79.61 - lr: 0.000781\n",
            "2020-12-23 07:02:15,919 epoch 48 - iter 32/43 - loss 2.37382200 - samples/sec: 79.02 - lr: 0.000781\n",
            "2020-12-23 07:02:17,489 epoch 48 - iter 36/43 - loss 2.35311627 - samples/sec: 81.63 - lr: 0.000781\n",
            "2020-12-23 07:02:19,125 epoch 48 - iter 40/43 - loss 2.36811108 - samples/sec: 78.28 - lr: 0.000781\n",
            "2020-12-23 07:02:20,013 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:02:20,016 EPOCH 48 done: loss 2.2843 - lr 0.0007813\n",
            "2020-12-23 07:02:21,057 DEV : loss 3.710690498352051 - score 0.7199\n",
            "2020-12-23 07:02:21,063 BAD EPOCHS (no improvement): 1\n",
            "2020-12-23 07:02:21,064 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:02:22,599 epoch 49 - iter 4/43 - loss 3.10217327 - samples/sec: 83.61 - lr: 0.000781\n",
            "2020-12-23 07:02:24,242 epoch 49 - iter 8/43 - loss 2.89496887 - samples/sec: 77.98 - lr: 0.000781\n",
            "2020-12-23 07:02:25,746 epoch 49 - iter 12/43 - loss 2.79698294 - samples/sec: 85.19 - lr: 0.000781\n",
            "2020-12-23 07:02:27,308 epoch 49 - iter 16/43 - loss 2.66036732 - samples/sec: 82.10 - lr: 0.000781\n",
            "2020-12-23 07:02:28,932 epoch 49 - iter 20/43 - loss 2.56851624 - samples/sec: 78.87 - lr: 0.000781\n",
            "2020-12-23 07:02:30,536 epoch 49 - iter 24/43 - loss 2.46998921 - samples/sec: 79.90 - lr: 0.000781\n",
            "2020-12-23 07:02:32,119 epoch 49 - iter 28/43 - loss 2.38675897 - samples/sec: 80.94 - lr: 0.000781\n",
            "2020-12-23 07:02:33,616 epoch 49 - iter 32/43 - loss 2.34852429 - samples/sec: 85.57 - lr: 0.000781\n",
            "2020-12-23 07:02:35,238 epoch 49 - iter 36/43 - loss 2.26214230 - samples/sec: 79.18 - lr: 0.000781\n",
            "2020-12-23 07:02:36,973 epoch 49 - iter 40/43 - loss 2.27751346 - samples/sec: 73.86 - lr: 0.000781\n",
            "2020-12-23 07:02:38,120 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:02:38,121 EPOCH 49 done: loss 2.3515 - lr 0.0007813\n",
            "2020-12-23 07:02:39,175 DEV : loss 3.7156431674957275 - score 0.7199\n",
            "2020-12-23 07:02:39,183 BAD EPOCHS (no improvement): 2\n",
            "2020-12-23 07:02:39,184 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:02:40,745 epoch 50 - iter 4/43 - loss 3.02254951 - samples/sec: 82.22 - lr: 0.000781\n",
            "2020-12-23 07:02:42,318 epoch 50 - iter 8/43 - loss 2.48994398 - samples/sec: 81.41 - lr: 0.000781\n",
            "2020-12-23 07:02:43,936 epoch 50 - iter 12/43 - loss 2.58591350 - samples/sec: 79.19 - lr: 0.000781\n",
            "2020-12-23 07:02:45,530 epoch 50 - iter 16/43 - loss 2.63728651 - samples/sec: 80.40 - lr: 0.000781\n",
            "2020-12-23 07:02:47,206 epoch 50 - iter 20/43 - loss 2.49545135 - samples/sec: 76.42 - lr: 0.000781\n",
            "2020-12-23 07:02:48,768 epoch 50 - iter 24/43 - loss 2.38368305 - samples/sec: 82.03 - lr: 0.000781\n",
            "2020-12-23 07:02:50,374 epoch 50 - iter 28/43 - loss 2.33652085 - samples/sec: 79.75 - lr: 0.000781\n",
            "2020-12-23 07:02:52,008 epoch 50 - iter 32/43 - loss 2.30103704 - samples/sec: 78.39 - lr: 0.000781\n",
            "2020-12-23 07:02:53,581 epoch 50 - iter 36/43 - loss 2.28795981 - samples/sec: 81.48 - lr: 0.000781\n",
            "2020-12-23 07:02:55,261 epoch 50 - iter 40/43 - loss 2.30364948 - samples/sec: 76.23 - lr: 0.000781\n",
            "2020-12-23 07:02:56,245 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:02:56,246 EPOCH 50 done: loss 2.3022 - lr 0.0007813\n",
            "2020-12-23 07:02:57,331 DEV : loss 3.7082459926605225 - score 0.7199\n",
            "2020-12-23 07:02:57,337 BAD EPOCHS (no improvement): 3\n",
            "2020-12-23 07:03:22,826 ----------------------------------------------------------------------------------------------------\n",
            "2020-12-23 07:03:22,836 Testing using best model ...\n",
            "2020-12-23 07:03:22,845 loading file /content/model/best-model.pt\n",
            "2020-12-23 07:04:24,064 0.6873\t0.6627\t0.6748\n",
            "2020-12-23 07:04:24,071 \n",
            "Results:\n",
            "- F1-score (micro) 0.6748\n",
            "- F1-score (macro) 0.4395\n",
            "\n",
            "By class:\n",
            "GPE.NAM    tp: 41 - fp: 12 - fn: 6 - precision: 0.7736 - recall: 0.8723 - f1-score: 0.8200\n",
            "GPE.NOM    tp: 0 - fp: 0 - fn: 2 - precision: 0.0000 - recall: 0.0000 - f1-score: 0.0000\n",
            "LOC.NAM    tp: 6 - fp: 9 - fn: 13 - precision: 0.4000 - recall: 0.3158 - f1-score: 0.3529\n",
            "LOC.NOM    tp: 0 - fp: 0 - fn: 9 - precision: 0.0000 - recall: 0.0000 - f1-score: 0.0000\n",
            "ORG.NAM    tp: 15 - fp: 16 - fn: 24 - precision: 0.4839 - recall: 0.3846 - f1-score: 0.4286\n",
            "ORG.NOM    tp: 6 - fp: 3 - fn: 11 - precision: 0.6667 - recall: 0.3529 - f1-score: 0.4615\n",
            "PER.NAM    tp: 89 - fp: 33 - fn: 24 - precision: 0.7295 - recall: 0.7876 - f1-score: 0.7574\n",
            "PER.NOM    tp: 120 - fp: 53 - fn: 52 - precision: 0.6936 - recall: 0.6977 - f1-score: 0.6957\n",
            "2020-12-23 07:04:24,072 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [15.872177124023438,\n",
              "  17.027008056640625,\n",
              "  9.663846015930176,\n",
              "  6.951746463775635,\n",
              "  6.2898736000061035,\n",
              "  5.883848667144775,\n",
              "  4.86557149887085,\n",
              "  4.57301664352417,\n",
              "  4.379792213439941,\n",
              "  4.182097911834717,\n",
              "  3.9913041591644287,\n",
              "  3.9053568840026855,\n",
              "  3.975558042526245,\n",
              "  3.875393867492676,\n",
              "  4.1406426429748535,\n",
              "  4.614320755004883,\n",
              "  3.71307373046875,\n",
              "  3.8614141941070557,\n",
              "  3.892859697341919,\n",
              "  3.6830766201019287,\n",
              "  4.1208062171936035,\n",
              "  3.6688966751098633,\n",
              "  3.964456796646118,\n",
              "  3.8440518379211426,\n",
              "  3.710570812225342,\n",
              "  3.776811361312866,\n",
              "  3.6381967067718506,\n",
              "  3.6773457527160645,\n",
              "  3.7301440238952637,\n",
              "  3.7477946281433105,\n",
              "  3.6717283725738525,\n",
              "  3.706446409225464,\n",
              "  3.7481448650360107,\n",
              "  3.5523171424865723,\n",
              "  3.80267333984375,\n",
              "  3.768207550048828,\n",
              "  3.7337565422058105,\n",
              "  3.701817035675049,\n",
              "  3.700406789779663,\n",
              "  3.7018449306488037,\n",
              "  3.700836658477783,\n",
              "  3.7391464710235596,\n",
              "  3.683899402618408,\n",
              "  3.7267754077911377,\n",
              "  3.747907876968384,\n",
              "  3.712575674057007,\n",
              "  3.726006031036377,\n",
              "  3.710690498352051,\n",
              "  3.7156431674957275,\n",
              "  3.7082459926605225],\n",
              " 'dev_score_history': [0.0049019607843137246,\n",
              "  0.0049504950495049506,\n",
              "  0.22495606326889278,\n",
              "  0.5,\n",
              "  0.5691275167785235,\n",
              "  0.5457516339869282,\n",
              "  0.6014388489208633,\n",
              "  0.6494845360824743,\n",
              "  0.6607142857142858,\n",
              "  0.6638537271448663,\n",
              "  0.690537084398977,\n",
              "  0.7108886107634543,\n",
              "  0.708860759493671,\n",
              "  0.6989528795811518,\n",
              "  0.6979038224414302,\n",
              "  0.6697566628041715,\n",
              "  0.7158948685857321,\n",
              "  0.7099143206854346,\n",
              "  0.7008547008547009,\n",
              "  0.7139303482587065,\n",
              "  0.6967895362663495,\n",
              "  0.718397997496871,\n",
              "  0.7110582639714624,\n",
              "  0.7233009708737864,\n",
              "  0.7205882352941176,\n",
              "  0.7235079171741778,\n",
              "  0.7293519695044473,\n",
              "  0.7243902439024389,\n",
              "  0.7255139056831922,\n",
              "  0.7206771463119709,\n",
              "  0.7108433734939759,\n",
              "  0.7177615571776156,\n",
              "  0.7064555420219245,\n",
              "  0.7286432160804021,\n",
              "  0.7103030303030304,\n",
              "  0.714975845410628,\n",
              "  0.7118644067796612,\n",
              "  0.7170731707317073,\n",
              "  0.7199017199017198,\n",
              "  0.7263803680981595,\n",
              "  0.7214723926380369,\n",
              "  0.7195121951219513,\n",
              "  0.7183271832718326,\n",
              "  0.7146341463414634,\n",
              "  0.7195121951219513,\n",
              "  0.7172582619339046,\n",
              "  0.7172582619339046,\n",
              "  0.7199017199017198,\n",
              "  0.7199017199017198,\n",
              "  0.7199017199017198],\n",
              " 'test_score': 0.6747868453105967,\n",
              " 'train_loss_history': [39.3918092860732,\n",
              "  20.239362927370294,\n",
              "  16.03036057671835,\n",
              "  12.817398980606434,\n",
              "  10.0896475925002,\n",
              "  8.61633418327154,\n",
              "  8.001074857490007,\n",
              "  7.203541162402131,\n",
              "  6.831127937449965,\n",
              "  6.476255522217861,\n",
              "  5.95285392362018,\n",
              "  5.596282105113184,\n",
              "  5.506079368813094,\n",
              "  5.423311372135961,\n",
              "  5.056821773218554,\n",
              "  4.694352033526399,\n",
              "  4.133306386858918,\n",
              "  3.928000960239144,\n",
              "  3.6805584042571313,\n",
              "  3.619281386220178,\n",
              "  3.5870109214339148,\n",
              "  3.1715654755054516,\n",
              "  3.2504708766937256,\n",
              "  3.1074951027714928,\n",
              "  2.977855638016102,\n",
              "  2.942379646523054,\n",
              "  2.881530854244565,\n",
              "  2.889226747113605,\n",
              "  2.737649867700976,\n",
              "  2.7821329859800117,\n",
              "  2.732440388479898,\n",
              "  2.619695785433747,\n",
              "  2.639809708262599,\n",
              "  2.545081159056619,\n",
              "  2.468545126360516,\n",
              "  2.444379066312036,\n",
              "  2.357311350661655,\n",
              "  2.4725295277528985,\n",
              "  2.4056211488191472,\n",
              "  2.371789370858392,\n",
              "  2.383048586200836,\n",
              "  2.3196271075758825,\n",
              "  2.310530956401381,\n",
              "  2.399727616199227,\n",
              "  2.2986392808514973,\n",
              "  2.3703589217607366,\n",
              "  2.2600344280863918,\n",
              "  2.2843340961045997,\n",
              "  2.351484160090602,\n",
              "  2.3021788763445477]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX1abswOTdfl"
      },
      "source": [
        "We see that the output accuracy (F1-score) for our new model is **67.5%** (F1-score (micro) 0.6748). We use micro F1-score (rather than macro F1-score) as there are multiple entity classes in this setup with [class imbalance](https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin).\r\n",
        "\r\n",
        "> We have a new SOTA NER model in mandarin, over 20 percentage points (absolute) better than the previous SOTA for this Weibo dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbaB4a27Tq7P"
      },
      "source": [
        "## Using the Model (Running Inference)\r\n",
        "\r\n",
        "Running the model to do some predictions/inference is as simple as calling `tagger.predict(sentence)`. Do note that for mandarin each character needs to be split with spaces between each character (e.g. `一 节 课 的 时 间`) so that the tokenizer will work properly to split them to tokens (if you're processing them for input into the model when building an app). For more information on this, check out the [flair tutorial on tokenization](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_1_BASICS.md#tokenization)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOtVbbI1Jsdz",
        "outputId": "1e299948-dcc3-40f5-f54f-13c0ee613b8a"
      },
      "source": [
        "from flair.data import Sentence\r\n",
        "from flair.models import SequenceTagger\r\n",
        "from flair.data import Corpus\r\n",
        "\r\n",
        "# Load the model that we trained, you can comment this out if you already have \r\n",
        "# the model loaded (e.g. if you just ran the training)\r\n",
        "tagger: SequenceTagger = SequenceTagger.load(\"/content/model/final-model.pt\")\r\n",
        "\r\n",
        "# Load the WEIBO corpus and use the first 5 sentences from the test set\r\n",
        "corpus = flair.datasets.WEIBO_NER()\r\n",
        "for idx in range(0, 5):\r\n",
        "  sentence = corpus.test[idx]\r\n",
        "  tagger.predict(sentence)\r\n",
        "  print(sentence.to_tagged_string())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-23 07:41:49,576 Reading data from /root/.flair/datasets/weibo_ner\n",
            "2020-12-23 07:41:49,577 Train: /root/.flair/datasets/weibo_ner/weiboNER_2nd_conll_format.train\n",
            "2020-12-23 07:41:49,578 Dev: /root/.flair/datasets/weibo_ner/weiboNER_2nd_conll_format.dev\n",
            "2020-12-23 07:41:49,579 Test: /root/.flair/datasets/weibo_ner/weiboNER_2nd_conll_format.test\n",
            "一 节 课 的 时 间 真 心 感 动 了 李 <B-PER.NAM> 开 <I-PER.NAM> 复 <E-PER.NAM> 感 动\n",
            "回 复 支 持 ， 赞 成 ， 哈 哈 米 八 吴 够 历 史 要 的 陈 <B-PER.NAM> 小 <I-PER.NAM> 奥 <E-PER.NAM> 丁 丁 我 爱 小 肥 肥 一 族 大 头 仔 大 家 团 结 一 致 ， 誓 要 去 台 <B-GPE.NAM> 湾 <E-GPE.NAM> 饮 喜 酒 ， 由 包 机 ， 团 结 的 力 量 大\n",
            "剑 网 乱 世 长 安 公 测 盛 典 今 日 开 启 ， 海 量 豪 礼 火 爆 开 送 精 美 挂 件 、 听 雨 · 汉 服 娃 娃 、 诙 谐 双 骑 独 轮 车 等 你 来 拿 ， 更 有 千 台 红 米 手 机 、 等 十 四 重 惊 喜 转 发 即 抽 活 动 地 址 ： 已 有 人 参 与 剑 网 官 方 微 博 剑 网 客 户 服 务\n",
            "在 街 上 听 见 音 乐 我 舞 动 起 来 很 丢 人 ？ 真 的 很 丢 人 吗 ？\n",
            "三 <B-PER.NAM> 毛 <E-PER.NAM> 说 我 唯 一 锲 而 不 舍 ， 愿 意 以 自 己 的 生 命 去 努 力 的 ， 只 不 过 是 保 守 我 个 人 的 心 怀 意 念 ， 在 我 有 生 之 日 ， 做 一 个 真 诚 的 人 ， 不 放 弃 对 生 活 的 热 爱 和 执 着 ， 在 有 限 的 时 空 里 ， 过 无 限 广\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_PbWkOxUoXk"
      },
      "source": [
        "We can connect to Google Drive with the following code to save any files you want to persist. You can also click the `Files` icon on the left panel and click `Mount Drive` to mount your Google Drive.\r\n",
        "\r\n",
        "The root of your Google Drive will be mounted to `/content/drive/My Drive/`. If you have problems mounting the drive, you can check out this [tutorial](https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGqgwwvkUuNv"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh9_zd6aUqMG"
      },
      "source": [
        "You can move the model files from our local directory to your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P7UpDkVUyQQ"
      },
      "source": [
        "import shutil\r\n",
        "shutil.move('/content/model/', \"/content/drive/My Drive/model/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bpu244PQUyso"
      },
      "source": [
        "More Notebooks @ [eugenesiow/practical-ml](https://github.com/eugenesiow/practical-ml) and do drop us some feedback on how to improve the notebooks on the [Github repo](https://github.com/eugenesiow/practical-ml/)."
      ]
    }
  ]
}